---
title: "Introduction et Révision des Concepts Essentiels des Séries Temporelles"
author: "Chapitre 1"
date: "Économétrie des séries temporelles"
output:
  beamer_presentation:
    number_sections: true
    toc: true
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{amssymb}
  - \setbeamertemplate{footline}{\hfill\raisebox{2pt}[0pt][0pt]\scriptsize{Économétrie des         Séries Temporelles - Chapitre 1}\hspace*{10pt} {\insertframenumber{} / \inserttotalframenumber}\hspace*{2pt}}
fontsize: 8pt
---

<!-- --- -->
<!-- title: "Introduction et Révision des Concepts Essentiels des Séries Temporelles" -->
<!-- author: "Chapitre 1" -->
<!-- date: "Économétrie des séries temporelles" -->
<!-- output: -->
<!--   xaringan::moon_reader: -->
<!--     css: [default, hygge-duke, hygge, custom_colors.css, custom_footer.css] -->
<!--     lib_dir: libs -->
<!--     nature: -->
<!--       ratio: "14:9" -->
<!--       highlightStyle: github -->
<!--       highlightLines: true -->
<!--       countIncrementalSlides: false -->
<!--       titleSlideClass: ["center", "middle", "my-title"] -->
<!--     mathjax: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" -->
<!-- --- -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  error = TRUE,
  fig.width = 10,
  fig.height = 5,
  dpi = 600,
  fig.keep = "last",
  message = FALSE,
  warning = FALSE,
  results = "markup"
)
```

<!-- layout:true -->
<!-- <div class="footer"><span>Économétrie des Séries Temporelles - Chapitre 1</span></div> -->

<!-- comme \vspace{} -->
<!-- <div style="margin-top: -0.5em;"></div> -->


---

## Ouvrages de référence

**Time Series Analysis**, James D. Hamilton

--

*Analysis of Financial Time Series*, Second Edition, Ruey S . Tsay 

*Introductory Econometrics for Finance*, Second Edition, Chris Brooks

*Econometrics analysis*, Greene

*Introductory Econometrics*, Second Edition, J. M. Wooldridge

*Applied Time Series Analysis A Practical Guide to Modeling and Forecasting*, Terence C. Mills

*Introductory Time Series with R*, Paul S.P. Cowpertwait and Andrew V. Metcalfe

*Applied Time Series Econometrics*, Edited by Helmut Lütkepohl and Markus Krätzig

--

**Time Series Analysis and Its Applications With R Examples**, Third edition, Robert H. Shumway and David S. Stoffer

---

# 1. Introduction
**Série temporelle** : une séquence d'observations à travers le temps.

**Objectif** de l'analyse des séries temporelles : formuler un modèle statistique qui est une représentation
    bien spécifiée du processus inconnu qui a généré la série temporelle
    observée.

---

## Taux de chômage aux États-Unis
<!-- fredr_set_key("229661c3a1cbd67e2388fafb768ca882") -->
```{r, echo=F}
library(fredr) #https://fred.stlouisfed.org/docs/api/api_key.html
fredr_set_key("229661c3a1cbd67e2388fafb768ca882")
```
```{r libs, echo=T}
library(fredr) #https://fred.stlouisfed.org/docs/api/api_key.html
#fredr_set_key("votre clé perso")
library(tseries)
library(astsa)
library(xts)
library(quantmod)
```

--

<div style="margin-top: -0.5em;"></div>

```{r Unrate, fig.width = 10, fig.height = 3.5}
USunrate = fredr(series_id = "UNRATE",
           observation_start = as.Date("1948-01-01"),
           observation_end = as.Date("2024-10-01"))
tsplot(USunrate$date, USunrate$value, 
       ylab="Taux de chomage aux USA", xlab='')
abline(h = mean(USunrate$value), col = "red", lty = 2, lwd = 2)
legend("topright", legend = "Moyenne", col = "red", lty = 2, lwd = 2, bty = "n")
```

---

## Taux de chômage aux États-Unis (suite)
### Un oeil sur les données
```{r exUnrateHeadTail}
head(USunrate)
tail(USunrate)
```

---

## Rendement du marché des titres du Trésor américain à échéance constante de 10 ans
```{r IRT10, fig.width = 10, fig.height = 4.0}
IRT10 = fredr(series_id = "GS10",
           observation_start = as.Date("2000-01-01"),
           observation_end = as.Date("2024-10-01"),
           frequency = "m")
tsplot(IRT10$date, IRT10$value, 
       ylab="Taux de rendement", xlab='')
```

---

# Structure du cours

Modèles empiriques

Définir les concepts statistiques clés

Autocorrélation et autocorrélation partielle

Stationnarité et ergodicité

Processus stationnaires

Processus non stationnaires

Données empiriques

---

# 2. Modèles Empiriques

Sorties expérimentales causées par des entrées :
    $$\underset{[\text{sortie}]}{y_t} = \underset{[\text{entrée}]}{f(z_t)} + \underset{[\text{perturbation}]}{\nu_t}$$

$y_t$ observé lorsque $z_t$ est en entrée ; $f(\cdot)$ mappe les entrées aux sorties ; $\nu_t$ est une petite perturbation aléatoire.

Les mêmes sorties se répètent en répétant les expériences avec les mêmes entrées.
    
---

## Modèles Empiriques (suite)

Dans un modèle économétrique, cependant :
    $$\underset{[\text{observée}]}{y_t} = \underset{[\text{explication}]}{g(z_t)} + \underset{[\text{reste}]}{\varepsilon_t}$$

$y_t$ décomposé en deux composantes : $g(z_t)$ (partie expliquée) et
    $\varepsilon_t$ (partie inexpliquée).

Toujours possible de décomposer $y_t$ même si $y_t$ ne dépend pas de
    $g(z_t)$.

---

## Modèles Empiriques (suite)

En économétrie : $$\varepsilon_t = y_t - g(z_t)$$

Les modèles peuvent être conçus par la sélection de $z_t$.

Les critères de conception doivent être analysés : conduiront à la notion de modèle congruent.

Les modèles congruents successifs doivent expliquer les précédents : concept d'englobement -  par lequel le progrès est réalisé.

---

## Modèles de Séries Temporelles

Processus autorégressifs univariés :
    $$y_t = \alpha y_{t-1} + \varepsilon_t, \quad \varepsilon_t \sim \mathsf{WN}(0, \sigma^2)$$

$\mathsf{WN}$ - bruit blanc : processus à moyenne nulle, variance constante

Séries temporelles multiples :
    $${\bf y}_t = {\bf A} y_{t-1} + \boldsymbol{\varepsilon}_t, \quad \boldsymbol{\varepsilon}_t \sim \mathsf{WN}(\boldsymbol{0}, \boldsymbol{\Sigma})$$

e.g. VAR :

$$\begin{bmatrix}
    y_{1,t} \\
    y_{2,t}
\end{bmatrix}
=
\begin{bmatrix}
    \alpha_{11} & \alpha_{12} \\
    \alpha_{21} & \alpha_{22}
\end{bmatrix}
\begin{bmatrix}
    y_{1,t-1} \\
    y_{2,t-1}
\end{bmatrix}
+
\begin{bmatrix}
    \varepsilon_{1,t} \\
    \varepsilon_{2,t}
\end{bmatrix}$$

$$\begin{bmatrix}
    \varepsilon_{1,t} \\
    \varepsilon_{2,t}
\end{bmatrix}
\sim \mathsf{WN}
\left(\begin{bmatrix}
    0 \\
    0
\end{bmatrix},
\begin{bmatrix}
    \sigma_1^2 & \sigma_{12} \\
    \sigma_{21} & \sigma_2^2
\end{bmatrix}
\right)$$

---

## Modèles de Séries Temporelles (suite)

<div style="margin-top: +3.2em;"></div>

Modèle autorégressif à retard distribué (ADL) :

$$y_t = \alpha_1 z_t + \alpha_2 y_{t-1} + \alpha_3 z_{t-1} + \varepsilon_t, \quad \varepsilon_t \sim \mathsf{WN}(0, \sigma^2)$$

---

# 3. Données de Séries Temporelles

Une **série temporelle** est une variable aléatoire observée de manière répétée dans le temps, indexée par $t$.

Une **variable aléatoire** dans le contexte des séries temporelles est une fonction $Y_t$ qui associe une valeur réelle à chaque instant $t$ dans un ensemble d'indices temporels $\mathcal{T}$ (par exemple, des jours, mois, ou années).

$Y_t : \Omega \to \mathbb{R}$, où $\Omega$ est l'ensemble des résultats possibles, et $t \in \mathcal{T}$.

À chaque instant $t$, $Y_t$ représente une observation aléatoire, influencée par des facteurs incertains.

Contrairement à des variables indépendantes, les variables aléatoires $Y_t$ dans une série temporelle présentent souvent des dépendances temporelles (un état passé influence les états futurs).

**Exemple** : $Y_t$ pourrait représenter la température quotidienne d'une ville, la valeur d'une action boursière, ou le nombre de ventes quotidiennes d'un produit.

---

## Processus Stochastiques

Un **processus stochastique** est une séquence ordonnée de **variables aléatoires** $\{y_t(\omega), \omega \in \Omega, t \in \mathcal{T}\}$ telle que
    pour chaque $t \in \mathcal{T}$, $y_t(\omega)$ est une variable aléatoire sur
    $\Omega$, et pour chaque $\omega \in \Omega$, $y_t(\omega)$ est une
    réalisation du processus stochastique sur l'ensemble d'indices $\mathcal{T}$.

Une série temporelle $\{y_t\}_{t=1}^T$ est une réalisation
    particulière $\{y_t\}_{t \in \mathcal{T}}$ d'un processus stochastique, une
    fonction $\mathcal{T} \rightarrow \mathbb{R}$ où $t \rightarrow y_t(\omega)$.

Le processus stochastique sous-jacent est dit avoir généré la série
    temporelle $y_1(\omega), y_2(\omega), \ldots, y_T(\omega)$, notée
    $\{y_t\}$.

---

## Caractéristiques des Séries Temporelles

**Persistance** - liée aux observations précédentes

**Retour à la moyenne** - retourne aux niveaux d'équilibre

**Autocorrélation** - pour décrire la dépendance temporelle

La distribution conjointe de $(y_t, y_{t-1}, \ldots, y_{t-h})$ est
    caractérisée par la

**Fonction d'Autocovariance** :
$$\begin{aligned}
\gamma_t(h) &= \mathsf{cov}(y_t, y_{t-h}) \\
            &= \mathsf{E}[(y_t - \mu_t)(y_{t-h} - \mu_{t-h})] \\
            &= \int \cdots \int (y_t - \mu_t)(y_{t-h} - \mu_{t-h}) f(y_t, \ldots, y_{t-h}) \, dy_t \cdots dy_{t-h}
\end{aligned}$$
où $\mu_t = \mathsf{E}[y_t] = \int y_t f(y_t) \, dy_t$ est la moyenne inconditionnelle de $y_t$.

L'indice $t$ capture la dépendance temporelle de l'autocovariance.

---

## Moments d'Échantillon

### Moyenne : 

$$\bar{y} = \frac{1}{T} \sum_{t=1}^T y_t$$

### Fonction d'Autocorrélation d'Échantillon (ACF) : 

$$\rho_t(h) = \frac{\sum_{t=h+1}^T (y_t - \bar{y})(y_{t-h} - \bar{y})}{\sum_{t=1}^T (y_t - \bar{y})^2}$$

---

## Moments d'Échantillon (suite)

### Fonction d'Autocorrélation Partielle (PACF) : 

$\mathsf{r}_{ij.k} = \mathsf{corr}(y_t, y_{t-h} | y_{t-1}, \ldots, y_{t-h+1})$

Dans le cas à 3 variables :
    $$\mathsf{r}_{02.1} = \frac{\mathsf{r}_{02} - \mathsf{r}_{01}\mathsf{r}_{12}}{\sqrt{1 - \mathsf{r}_{01}^2}\sqrt{1 - \mathsf{r}_{12}^2}}$$
    où
$$\begin{aligned}
\mathsf{r}_{01} &= \mathsf{corr}(y_t, y_{t-1}) = \mathsf{corr}(y_{t-1}, y_{t-2}) = \mathsf{r}_{12} = \rho(1) \\
\mathsf{r}_{02} &= \mathsf{corr}(y_t, y_{t-2}) = \rho(2)
\end{aligned}$$
Donc : 
$$\mathsf{r}_{02.1} = \frac{\rho(2) - \rho^2(1)}{1 - \rho^2(1)}$$ 
sous moyenne et variance constantes.

---

## Moments d'Échantillon (suite)

### Processus AR(p) :

$\mathsf{r}_{ij.k} = 0$ pour $h > p$

Indépendance conditionnelle

Pour $h = 2$ :
    $$\mathsf{r}_{02.1} = \mathsf{corr}(y_t, y_{t-2} | y_{t-1}) = 0$$ si
    $$f(y_t, y_{t-2} | y_{t-1}) = f(y_t | y_{t-1}) f(y_{t-2} | y_{t-1}).$$

Donc :
$$\begin{aligned}
f(y_t, y_{t-2} | y_{t-1}) &= f(y_t | y_{t-1}, y_{t-2}) f(y_{t-2} | y_{t-1})\\
                          &= f(y_t | y_{t-1})
\end{aligned}$$

**Processus de Markov** – la distribution conditionnelle de $y_t$
    étant donnée tout le passé $y_{t-1}, y_{t-2}, \ldots$, dépend seulement du
    passé immédiat $y_{t-1}$.

---

## Distributions des Moments d'Échantillon

Pour $y_t \sim \mathsf{IID}(\mu, \sigma^2)$ :
    $$\sqrt{T} \bar{y} \rightarrow N(\mu, \sigma^2)$$
    $$\sqrt{T} \hat{\rho}_h \rightarrow N(0, 1)$$
    $$\sqrt{T} \hat{r}_{ij.k} \rightarrow N(0, 1)$$ 
    où $\mathsf{IID}$ - indépendamment et identiquement distribué.

---

## ACF/PACF pour le taux de chômage aux USA

```{r acf1, results='hide', fig.width = 10, fig.height = 4.5}
par(mfrow=c(3,1))
tsplot(USunrate$date, USunrate$value, 
       ylab="Taux de chomage aux USA", xlab='')
acf(USunrate$value, main = "ACF USunrate")
pacf(USunrate$value, main = "PACF USunrate")
```

---

## ACF/PACF pour $\Delta$ taux de chômage aux USA

```{r acf2, results='hide', fig.width = 10, fig.height = 4.5}
par(mfrow=c(3,1))
tsplot(USunrate$date[-1], diff(USunrate$value), 
       ylab=expression(Delta ~ "Taux de chomage aux USA"), xlab='')
acf(diff(USunrate$value), main = "")
pacf(diff(USunrate$value), main = "")
```

---

## Stationnarité

Une série temporelle $\{y_t\}$ est **faiblement stationnaire** ou
    stationnaire en covariance si les premiers et seconds moments du
    processus existent et sont invariants dans le temps.
    $$\mathsf{E}[y_t] = \mu < \infty \quad \forall t \in T$$
    $$\mathsf{E}[(y_t - \mu)(y_{t-h} - \mu)] = \gamma(h) < \infty \quad \forall t, h$$

La stationnarité implique $\gamma_t(h) = \gamma_t(-h) = \gamma(h)$.

--

<div style="margin-top: 1.5em;"></div>

Une série temporelle $\{y_t\}$ est **strictement stationnaire** si, pour
    toute valeur de $h_1, h_2, \ldots, h_n$, la distribution jointe
    de $y_t, y_{t+h}, \ldots, y_{t+hn}$ dépend uniquement des
    intervalles $h_1, h_2, \ldots, h_n$ et non de $t$ :
    $$f(y_t, y_{t+h}, \ldots, y_{t+hn}) = f(y_\tau, y_{\tau+h}, \ldots, y_{\tau+hn}) \quad \forall t, \tau$$

La stationnarité stricte implique que tous les moments sont invariants dans le temps.

---

## Dow Jones Industrial Average

```{r DJIA, results='hide', fig.width = 10, fig.height = 4.0}
DJI = getSymbols("^DJI", src = "yahoo", from = "2004-01-01", 
                 to = "2014-01-01", auto.assign = FALSE)
djia = Cl(DJI)
djiar = 100 * diff(log(djia))[-1]  # [-1] retire 1 observation
par(mfrow=c(4,1))
tsplot(index(DJI), djia, main="Indice DJIA", ylab="Indice")
tsplot(index(DJI)[-1], djiar, main="Rendement DJIA", ylab="Taux de rendement")
acf(djiar, main = '')
pacf(djiar, main = '')
```

---
 
## Ergodicité

L'ergodicité concerne l'information qui peut être dérivée d'une
    moyenne temporelle sur la moyenne commune à un point dans le temps.
    (La loi des grands nombres faible n'est pas applicable car la série
    temporelle observée représente une seule réalisation du processus
    stochastique).

Soit $\{y_t(\omega), \omega \in \Omega, t \in T\}$ un processus
    faiblement stationnaire, tel que $\mathsf{E}[y_t(\omega)] = \mu < \infty$ et
    $\mathsf{E}[(y_t(\omega) - \mu)^2] = \sigma_y^2 < \infty$, $\forall t$, et
    $$\bar{y}_T = \frac{1}{T} \sum_{t=1}^T y_t$$ est la moyenne
    temporelle.

Si $\bar{y}_T \xrightarrow{p} \mu$ lorsque $T \to \infty$, $\{y_t\}$
    est ergodique pour la moyenne.

Ergodicité - indépendance asymptotique

Stationnarité - invariance temporelle

---

## Ergodicité (suite)

L'ergodicité nécessite que la mémoire d'un processus stochastique
    s'estompe de sorte que la covariance entre des observations de plus
    en plus distantes converge vers 0 suffisamment rapidement.

Pour les processus stationnaires,
    $$\sum_{h=0}^\infty |\gamma(h)| < \infty$$ est suffisant pour
    assurer l'ergodicité.

Ergodicité pour les seconds moments :
    $$\hat{\gamma}(h) = \frac{1}{T-h+1} \sum_{t=h+1}^T (y_t - \mu)(y_{t-h} - \mu) \xrightarrow{p} \gamma(h)$$

---

# 4. Processus de Base

## Bruit blanc

Un processus de bruit blanc est un processus faiblement stationnaire
    qui a une moyenne nulle et est non corrélé dans le temps :
    $$u_t \sim \mathsf{WN}(0, \sigma^2)$$

$\{u_t\}$ est $\mathsf{WN}$ si $\mathsf{E}[u_t] = 0$, $E[u_t^2] = \sigma^2 < \infty$, et
    $\mathsf{E}[u_t u_{t-h}] = 0$ où $h \neq 0$ et $t-h \in T$ pour tout
    $t \in T$.

Si la variance constante est relâchée à $\mathsf{E}[u_t^2] < \infty$,
    $\{u_t\}$ est un processus de bruit blanc faible.

Si $\{u_t\}$ est normalement distribué, c'est un processus de bruit
    blanc gaussien : $$u_t \sim \mathsf{NID}(0, \sigma^2)$$

Normalité $\Rightarrow$ stationnarité stricte et indépendance
    sérielle.

---

## Génération de bruits blancs gaussiens centrés réduits

```{r whitenoise}
par(mfrow=c(2,2))
tsplot(rnorm(30)); tsplot(rnorm(100))
tsplot(rnorm(1000)); tsplot(rnorm(5000))
```

---

## Processus $\mathsf{IID}$

Un processus $\{u_t\}$ avec des variables indépendantes et
    identiquement distribuées est $\mathsf{IID}$ : $$u_t \sim \mathsf{IID}(0, \sigma^2)$$

---

## Martingales

Le processus stochastique $\{y_t\}$ est une martingale par rapport à
    un ensemble d'informations $I_{t-1}$ des données
    $\{I_{t-1} = y_{t-1}, \ldots, y_1\}$, réalisé à $t-1$, si
    $\mathsf{E}[|y_t|] < \infty$ et l'espérance conditionnelle
    $\mathsf{E}[y_t | I_{t-1}] = y_{t-1}$.

Le processus $\{u_t = y_t - y_{t-1}\}$ avec $\mathsf{E}[|u_t|] < \infty$ et
    $\mathsf{E}[u_t | I_{t-1}] = 0$ est une séquence de différences de martingale
    (MDS).

---

## Innovations

Une innovation $\{u_t\}$ contre un ensemble d'informations $I_{t-1}$
    est un processus dont la densité $f(u_t | I_{t-1})$ ne dépend pas de
    $I_{t-1}$.

$\{u_t\}$ est une innovation de moyenne contre un ensemble
    d'informations $I_{t-1}$ si $\mathsf{E}[u_t | I_{t-1}] = 0$.

Une innovation $\{u_t\}$ doit être $\mathsf{WN}(0, \sigma^2$) si $I_{t-1}$
    contient l'historique $(U_{t-1} = u_0, u_1, \ldots, u_{t-1})$ de
    $u_t$, mais pas inversement.

Par conséquent, une innovation doit être une MDS.

---

## Processus Intégrés

Les processus intégrés peuvent être rendus stationnaires par
    différenciation.

Un processus stochastique $\{y_t\}$ est une marche aléatoire si :
    $$y_t = y_{t-1} + u_t \quad \text{pour} \quad t > 0 \quad \text{et} \quad y_0 = 0$$
    où $u_t \sim \mathsf{\mathsf{IID}}(0, \sigma^2)$, $\forall t > 0$.

Une marche aléatoire est non stationnaire et donc non ergodique :
    $$y_t = y_0 + \sum_{s=1}^t u_s \quad \forall t > 0.$$

La moyenne est invariante dans le temps :
    $$\mu = \mathsf{E}[y_t] = \mathsf{E}\left[y_0 + \sum_{s=1}^t u_s\right] = y_0 + \sum_{s=1}^t \mathsf{E}[u_s] = 0$$

---

## Processus Intégrés (suite)

Mais les seconds moments divergent. La variance est :
    $$\gamma_t(0) = \mathsf{E}[y_t^2] = \mathsf{E}\left[\left(y_0 + \sum_{s=1}^t u_s\right)^2\right] = \sum_{s=1}^t \sigma^2 = t\sigma^2$$

Les autocovariances sont :
    $$\gamma_t(h) = \mathsf{E}[y_t y_{t-h}] = \mathsf{E}\left[\left(y_0 + \sum_{s=1}^t u_s\right)\left(y_0 + \sum_{k=1}^{t-h} u_k\right)\right] = (t-h)\sigma^2$$
    pour tout $h > 0$.

---

## Processus Intégrés (suite)

La fonction d'autocorrélation est :
    $$\rho_t(h) = \frac{\gamma_t(h)}{\gamma_t(0) \gamma_{t-h}(0)} = \frac{(t-h)\sigma^2}{t\sigma^2 (t-h)\sigma^2} = 1 - \frac{h}{t}$$
    pour tout $h > 0$.

Si un processus stochastique doit être différencié $d$ fois pour
    atteindre la stationnarité, il est intégré d'ordre $d$, ou $I(d)$.

Une marche aléatoire est $I(1)$ et les processus stationnaires sont $I(0)$.
    
---

## Marche aléatoire avec et sans drift

<div style="margin-top: -1.2em;"></div>

```{r randomwalk, fig.width = 10, fig.height = 3.9, fig.cap="<div style='margin-top: -1.2em;'></div>Marche aléatoire, $\\sigma_w = 1$, avec dérive $y_0 = 0.2$ (ligne noire), sans dérive, $y_0 = 0$ (ligne bleue), et une ligne droite pointillée avec une pente de $0.2$."}
set.seed(154) # pour la reproductibilité
w = rnorm(200); x = cumsum(w) # deux commandes en une ligne
wd = w +.2;    xd = cumsum(wd)
par(mfrow=c(1,1))
tsplot(xd, ylim=c(-5,55), main="Marche aleatoire", ylab='')
lines(x, col=4) 
clip(0,200,0,50)
abline(h=0, col=4, lty=2)
abline(a=0, b=.2, lty=2)
```

<div style="margin-top: -1.2em;"></div>
Marche aléatoire, $\sigma_w = 1$, avec dérive $y_0 = 0.2$ (ligne noire), sans dérive, $y_0 = 0$ (ligne bleue), et une ligne droite pointillée avec une pente de $0.2$.

