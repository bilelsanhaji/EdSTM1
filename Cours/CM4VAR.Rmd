---
title: "Séries Temporelles Multivariées"
author: "Chapitre 4"
date: "Économétrie des séries temporelles"
output:
  beamer_presentation:
    number_sections: true
    toc: true
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{amssymb}
  - \setbeamertemplate{footline}{\hfill\raisebox{2pt}[0pt][0pt]\scriptsize{Économétrie des         Séries Temporelles - Chapitre 4}\hspace*{10pt} {\insertframenumber{} /                     \inserttotalframenumber}\hspace*{2pt}
    }
fontsize: 8pt
---

<!-- --- -->
<!-- title: "Racines unitaires" -->
<!-- author: "Chapitre 3" -->
<!-- date: "Économétrie des séries temporelles" -->
<!-- output: -->
<!--   xaringan::moon_reader: -->
<!--     css: [default, hygge-duke, hygge, custom_colors.css, custom_footer.css] -->
<!--     lib_dir: libs -->
<!--     nature: -->
<!--       ratio: "14:9" -->
<!--       highlightStyle: github -->
<!--       highlightLines: true -->
<!--       countIncrementalSlides: false -->
<!--       titleSlideClass: ["center", "middle", "my-title"] -->
<!--     mathjax: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" -->
<!-- --- -->


<!-- Mise en forme et espacements -->
<!-- layout:true -->
<!-- <div class="footer"><span>Économétrie des Séries Temporelles - Chapitre 3</span></div> -->

<!-- comme \vspace{} -->
<!-- <div style="margin-top: -0.5em;"></div> -->
<!-- &nbsp;-->



```{r setup, include=FALSE}
knitr::opts_chunk$set(
  #eval = FALSE,
  echo = TRUE,
  error = TRUE,
  fig.width = 10, 
  fig.height = 5,
  dpi = 600,
  fig.keep ="last",
  message = FALSE,
  warning = FALSE,
  results = "markup"
)
```

```{r, echo=FALSE}
library(quantmod)
library(fredr)
fredr_set_key("229661c3a1cbd67e2388fafb768ca882")
library(vars)
library(forecast)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(urca)
library(astsa)
library(aTSA)
library(forecast)
library(ppcor)
library(MASS)
```

---

# Structure du cours : 

Autorégressions Vectorielles -- VAR

Exemples de base

Propriétés 

- Stationnarité

Revisiter les processus ARMA univariés

Prévision
    
- Causalité de Granger
- Fonctions de réponse impulsionnelle

Cointégration

- Examiner les relations à long terme
- Déterminer si un VAR est cointégré
- Modèles de Correction d'Erreur
- Tests de Cointégration - Engle-Granger

---

## Autorégressions Vectorielles -- VAR

---

# Pourquoi l'analyse VAR ?

## VAR Stationnaires
- Déterminer si les variables se rétroagissent les unes sur les autres
- Améliorer les prévisions
- Modéliser l'effet d'un choc dans une série sur une autre
- Différencier entre les dynamiques à court terme et à long terme

## Cointégration
- Lier les marches aléatoires
- Découvrir les relations à long terme
- Peut améliorer considérablement les prévisions à moyen et long terme

---

# Définition du VAR

**Autorégression d'ordre P, AR(P)** :
\[ y_t = \phi_0 + \phi_1 y_{t-1} + \ldots + \phi_P y_{t-p} + \varepsilon_t \]

**Autorégression vectorielle d'ordre P, VAR(P)** :
\[ \mathbf{y}_t = \Phi_0 + \Phi_1 \mathbf{y}_{t-1} + \ldots + \Phi_P \mathbf{y}_{t-p} + \boldsymbol{\varepsilon}_t \]

où \( \mathbf{y}_t \) et \( \boldsymbol{\varepsilon}_t \) sont des vecteurs de dimension \( k \times 1 \) &nbsp;

**VAR(1) bivarié** :
\[
\begin{pmatrix}
y_{1,t} \\
y_{2,t}
\end{pmatrix}
=
\begin{pmatrix}
\phi_{01} \\
\phi_{02}
\end{pmatrix}
+
\begin{pmatrix}
\phi_{11} & \phi_{12} \\
\phi_{21} & \phi_{22}
\end{pmatrix}
\begin{pmatrix}
y_{1,t-1} \\
y_{2,t-1}
\end{pmatrix}
+
\begin{pmatrix}
\varepsilon_{1,t} \\
\varepsilon_{2,t}
\end{pmatrix}
\]

**Exprime de manière compacte deux modèles liés** :
\[ y_{1,t} = \phi_{01} + \phi_{11} y_{1,t-1} + \phi_{12} y_{2,t-1} + \varepsilon_{1,t} \]
\[ y_{2,t} = \phi_{02} + \phi_{21} y_{1,t-1} + \phi_{22} y_{2,t-1} + \varepsilon_{2,t} \]

---

# Stationnarité Revisitée

La stationnarité est une forme statistiquement significative de régularité. 

Un processus stochastique ${y_t}$ est stationnaire en covariance si :

- $E[y_t] = \mu, \quad \forall t$ 
- $V[y_t] = \sigma^2, \quad \sigma^2 < \infty,\; \forall t$ 
- $E[(y_t - \mu)(y_{t-s} - \mu)] = \gamma_s, \quad\forall t,s$

**Stationnarité de l'AR(1)** : $y_t = \phi y_{t-1} + \varepsilon_t$

- $\mid\phi\mid < 1$
- $\varepsilon_t$ est un bruit blanc

**Stationnarité de l'AR(P)** : $y_t = \phi_1 y_{t-1} + \ldots + \phi_P y_{t-P} + \varepsilon_t$

- Les racines de $(z^P - \phi_1 z^{P-1} - \phi_2 z^{P-2} - \ldots - \phi_{P-1} z - \phi_P)$ sont inférieures à 1
- $\varepsilon_t$ est un bruit blanc

*Pas de dépendance à* $t$

---

# Relation avec AR

## AR(1)

$$\begin{aligned}
y_t &= \phi_0 + \phi_1 y_{t-1} + \varepsilon_t\\
    &= \phi_0 + \phi_1 (\phi_0 + \phi_1 y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t \\
    &= \phi_0 + \phi_1 \phi_0 + \phi_1^2 y_{t-2} + \phi_1 \varepsilon_{t-1} + \varepsilon_t \\
    &= \phi_0 + \phi_1 \phi_0 + \phi_1^2 (\phi_0 + \phi_1 y_{t-3} + \varepsilon_{t-2}) + \phi_1 \varepsilon_{t-1} + \varepsilon_t \\
    & \vdots \\
    &= \sum_{i=0}^{\infty} \phi_1^i \phi_0 + \sum_{i=0}^{\infty} \phi_1^i \varepsilon_{t-i} \\
    &= (1 - \phi_1)^{-1} \phi_0 + \sum_{i=0}^{\infty} \phi_1^i \varepsilon_{t-i}
\end{aligned}$$

---

# Relation avec AR

## VAR(1)

$$\begin{aligned}
\mathbf{y}_t &= \boldsymbol{\Phi}_0 + \boldsymbol{\Phi}_1 \mathbf{y}_{t-1} + \boldsymbol{\varepsilon}_t\\
    &= \boldsymbol{\Phi}_0 + \boldsymbol{\Phi}_1 (\boldsymbol{\Phi}_0 + \boldsymbol{\Phi}_1 \mathbf{y}_{t-2} + \boldsymbol{\varepsilon}_{t-1}) + \boldsymbol{\varepsilon}_t \\
    &= \boldsymbol{\Phi}_0 + \boldsymbol{\Phi}_1 \boldsymbol{\Phi}_0 + \boldsymbol{\Phi}_1^2 \mathbf{y}_{t-2} + \boldsymbol{\Phi}_1 \boldsymbol{\varepsilon}_{t-1} + \boldsymbol{\varepsilon}_t \\
    &= \boldsymbol{\Phi}_0 + \boldsymbol{\Phi}_1 \boldsymbol{\Phi}_0 + \boldsymbol{\Phi}_1^2 (\boldsymbol{\Phi}_0 + \boldsymbol{\Phi}_1 \mathbf{y}_{t-3} + \boldsymbol{\varepsilon}_{t-2}) + \boldsymbol{\Phi}_1 \boldsymbol{\varepsilon}_{t-1} + \boldsymbol{\varepsilon}_t \\
    & \vdots \\
    &= \sum_{i=0}^{\infty} \boldsymbol{\Phi}_1^i \boldsymbol{\Phi}_0 + \sum_{i=0}^{\infty} \boldsymbol{\Phi}_1^i \boldsymbol{\varepsilon}_{t-i} \\
    &= (\mathbf{I}_k - \boldsymbol{\Phi}_1)^{-1} \boldsymbol{\Phi}_0 + \sum_{i=0}^{\infty} \boldsymbol{\Phi}_1^i \boldsymbol{\varepsilon}_{t-i}
\end{aligned}$$

---

# Propriétés d'un VAR(1) et AR(1)

## Moyenne, Variance, Autocovariance

$$
\begin{array}{lll}
\textbf{Propriété} & \textbf{AR(1)} & \textbf{VAR(1)} \\
\text{Équation} & y_t = \phi_0 + \phi_1 y_{t-1} + \varepsilon_t & \mathbf{y}_t = \boldsymbol{\Phi}_0 + \boldsymbol{\Phi}_1 \mathbf{y}_{t-1} + \boldsymbol{\varepsilon}_t \\
\text{Moyenne} & \frac{\phi_0}{1 - \phi_1} & (\mathbf{I}_k - \boldsymbol{\Phi}_1)^{-1} \boldsymbol{\Phi}_0 \\
\text{Variance} & \frac{\sigma^2}{1 - \phi_1^2} & (\mathbf{I} - \boldsymbol{\Phi}_1 \otimes \boldsymbol{\Phi}_1)^{-1} \text{vec}(\boldsymbol{\Sigma}) \\
\text{Autocovariance } s & \gamma_s = \phi_1^s V[y_t] & \boldsymbol{\Gamma}_s = \boldsymbol{\Phi}_1^s V[\mathbf{y}_t] \\
\text{Autocovariance } -s & \gamma{-s} = \phi_1^s V[y_t] & \boldsymbol{\Gamma}{-s} = V[\mathbf{y}_t] \boldsymbol{\Phi}_1^{s\prime} \\
\end{array}
$$

Les autocovariances des processus vectoriels ne sont pas symétriques, mais $\boldsymbol{\Gamma}_s = \boldsymbol{\Gamma}_{-s}'$

## Stationnarité

- AR(1) : $|\phi_1| < 1$
- VAR(1) : $|\lambda_i| < 1$ où $\lambda_i$ sont les valeurs propres de $\boldsymbol{\Phi}_1$

---

# VAR -- Relation PIB / Chômage

## Modèle

\[
\begin{pmatrix}
\text{PIB}_t \\
\text{CHO}_t
\end{pmatrix}
=
\begin{pmatrix}
0.547^{\star\star\star} \\
0.049^{\star}
\end{pmatrix}
+
\begin{pmatrix}
0.182^{\star\star} & -0.639^{\star\star\star} \\
-0.096^{\star\star\star} & 0.507^{\star\star\star}
\end{pmatrix}
\begin{pmatrix}
\text{PIB}_{t-1} \\
\text{CHO}_{t-1}
\end{pmatrix}
+
\begin{pmatrix}
\varepsilon_{1,t} \\
\varepsilon_{2,t}
\end{pmatrix}
\]

Significance : $\star\star\star : 1\%,\; \star\star : 5\%, \star : 10\%$

--- 

# Modèles

&nbsp;

**Modèle du PIB**
\[
\text{PIB}_t = \phi_{01} + \phi_{11,1} \text{PIB}_{t-1} + \phi_{12,1} \text{CHO}_{t-1} + \varepsilon_{1,t}
\]

**Modèle du Chomage**
\[
\text{CHO}_t = \phi_{02} + \phi_{21,1} \text{PIB}_{t-1} + \phi_{22,1} \text{CHO}_{t-1} + \varepsilon_{2,t}
\]

**Estimations VAR(1)**
\[
\text{PIB}_t = 0.547^{\star\star\star}  + 0.182^{\star\star}      \text{PIB}_{t-1} -0.639^{\star\star\star} \text{CHO}_{t-1} + \varepsilon_{1,t}
\]
\[
\text{CHO}_t = 0.049^{\star}            -0.096^{\star\star\star}  \text{PIB}_{t-1} + 0.507^{\star\star\star} \text{CHO}_{t-1} + \varepsilon_{2,t}
\]

**Estimations AR(1)**
\[
\text{PIB}_t = 0.648^{\star\star\star}  + 0.414^{\star\star\star}\text{PIB}_{t-1} \quad\quad\quad\quad\quad\quad\quad\quad  + \varepsilon_{1,t}
\]
\[
\text{CHO}_t = -0.003^{}               \quad\quad\quad\quad \quad\quad\quad\quad      + 0.692^{\star\star\star} \text{CHO}_{t-1} + \varepsilon_{2,t}
\]

---

# Données

$y_1$ : **PIB**
  
$y_2$ : **Chomâge**
  
Pays : **USA**
  
Période : **jan. 1980 - déc. 2019**
  
Fréquence : **trimestrielle**

---

# Les données -- Obtention 

```{r}
start_date <- as.Date("1980-01-01")
#start_date <- as.Date("1957-01-01")
end_date <- as.Date("2019-12-31")
#end_date <- as.Date("2024-12-31")

# 1. Télécharger et transormer les données FRED
gdp_data <- fredr(series_id = "GDPC1", 
                  observation_start = start_date, 
                  observation_end = end_date, frequency = "q")
unemp_data <- fredr(series_id = "UNRATE", 
                    observation_start = start_date, 
                    observation_end = end_date, frequency = "q")
```


---

# Graphs -- données brutes

```{r graph_pib_cho, echo=F}
par(mfrow = c(2, 1))
tsplot(gdp_data$date, gdp_data$value, main = "PIB", ylab = " ", xlab = " ")
tsplot(unemp_data$date, unemp_data$value, main = "CHO", ylab = " ", xlab = " ")
```

---

# Les données -- Manipulation

```{r}
# merge les deux variables dans une data.frame
data <- merge(gdp_data[, c("date", "value")], 
              unemp_data[, c("date", "value")],
              by = "date", suffixes = c("_gdp", "_unemp"))
colnames(data) <- c("date", "gdp", "unemp")

# transformation des variables
data <- data %>%
  mutate(log_gdp = log(gdp), unemp = unemp) %>%
  select(date, log_gdp, unemp)

# s'assurer que les données sont stationnaires (différencier si nécessaire)
data_diff <- data %>%
  mutate(d_log_gdp = c(NA, diff(log_gdp)),
         d_unemp = c(NA, diff(unemp))) %>%
  na.omit()

# autre méthode pour transformer les données (à faire). 

```

---

# Les données -- Tester la stationarité 

**PIB** -- log et diff(log)

```{r}
tseries::adf.test(data$log_gdp)
tseries::adf.test(data_diff$d_log_gdp)
```

---

# Les données -- Tester la stationarité 

**CHO** -- taux et variation de taux

```{r}
tseries::adf.test(data$unemp)
tseries::adf.test(data_diff$d_unemp)
```

---

# Les modèles -- Estimation VAR(1) vs AR(1)

```{r}
# 2. Estimation des modèles
# (a) Modèle VAR(1) multivarié
data_diff$d_log_gdp <- data_diff$d_log_gdp*100
# *100 pour etre en % et coller à l'echelle de la variation du taux de chomage
var_model <- vars::VAR(data_diff[, c("d_log_gdp", "d_unemp")],
                       p = 1, type = "const")

# (b) Modèles AR(1) univariés
ar_gdp <- arima(data_diff$d_log_gdp, order = c(1, 0, 0))
ar_unemp <- arima(data_diff$d_unemp, order = c(1, 0, 0))
```

---

# VAR(1) -- Résultats d'estimation

```{r}
# 3. Analyse des résultats
# Résultats du VAR(1)

# lancer tout de meme summary dans le rmd
# summary(var_model)

coeftest(var_model)
```

---

# Graph des résidus

```{r, fig.height = 4.7, fig.keep = 'all', echo = FALSE}
par(mfrow=c(2,1))
plot(var_model)
```

--- 

# ACF Résidus
```{r}
acf(residuals(var_model))
```

--- 

# PACF Résidus

```{r}
pacf(residuals(var_model))
```

---

# AR(1) -- Résultats d'estimation

```{r}
# Résultats des modèles AR(1)
coeftest(ar_gdp)
coeftest(ar_unemp)
```

---

# Résidus VAR(1) vs AR(1)

```{r, fig.keep='last', echo = FALSE}
# Comparer les résidus
residuals_var <- residuals(var_model)
residuals_ar_gdp <- residuals(ar_gdp)
residuals_ar_unemp <- residuals(ar_unemp)

par(mfrow=c(2,1))
tsplot(data_diff$date[-1], residuals_var[,1], xlab = "", ylab = "", main = "Résidus PIB")
lines(data_diff$date[-1], residuals_ar_gdp[-1], col=2)
legend("topright", legend=c("Résidus VAR(1)", "Résidus   AR(1)"), col=c(1,2), lty=1, bty = "n")
tsplot(data_diff$date[-1], residuals_var[,2], xlab = "", ylab = "", main = "Résidus CHO")
lines(data_diff$date[-1], residuals_ar_unemp[-1], col=2)
legend("topright", legend=c("Résidus VAR(1)", "Résidus   AR(1)"), col=c(1,2), lty=1, bty = "n")
```
---

# Prévision VAR(1)

```{r, echo=FALSE}
forecast_var <- predict(var_model, n.ahead = 10)
plot(forecast_var)
```

---

# Prévision AR(1)

```{r, echo=FALSE}
forecast_gdp <- forecast::forecast(ar_gdp, h = 10)
forecast_unemp <- forecast::forecast(ar_unemp, h = 10)

par(mfrow=c(2,1))
plot(forecast_gdp)
plot(forecast_unemp)
par(mfrow=c(1,1))
```

---

# Politique monétaire - VAR
## ~ Taylor Rule : Chomage, Fed Funds, Inflation

$y_1$ : **Chomage**, 
$y_2$ : **Fed Funds**
$y_3$ : **Inflation**
Pays : **USA**
Période : **jan. 1957 - déc. 2019**
Fréquence : **trimestrielle**

```{r, fig.keep='all'}
start_date <- as.Date("1957-01-01")
end_date <- as.Date("1999-12-31")
# start_date <- as.Date("1980-01-01")
# end_date <- as.Date("2019-12-31")

UNRATE <- fredr(series_id = "UNRATE", 
                observation_start = start_date, 
                observation_end = end_date, 
                frequency = "q")
DFF <- fredr(series_id = "DFF", 
             observation_start = start_date, 
             observation_end = end_date, 
             frequency = "q")
GDPDEF <- fredr(series_id = "GDPDEF", 
                observation_start = start_date, 
                observation_end = end_date, 
                frequency = "q")
```

---

# Stationarité -- visuelle

```{r, echo=FALSE}
par(mfrow=c(3,1))
tsplot(UNRATE$date, UNRATE$value, main = "UNRATE", ylab = " ", xlab = " ")
tsplot(DFF$date, DFF$value, main = "FF", ylab = " ", xlab = " ")
tsplot(GDPDEF$date, GDPDEF$value, , main = "INFL", ylab = " ", xlab = " ")
```

---

# Stationarité -- test

```{r} 
adf_unrate <- tseries::adf.test(UNRATE$value)
adf_dff <- tseries::adf.test(DFF$value)
adf_gdpdef <- tseries::adf.test(GDPDEF$value)

adf_values <- data.frame(
  Série = c("UNRATE", "DFF", "GDPDEF"),
  `Statistique ADF` = c(adf_unrate$statistic, 
                        adf_dff$statistic, 
                        adf_gdpdef$statistic),
  `p-value` = c(adf_unrate$p.value, 
                adf_dff$p.value, 
                adf_gdpdef$p.value)
)
adf_values
```

---

# Différenciation des séries

```{r}
# on différencie avec des méthodes qui maintiennent 
# une "cohérence" des nouvelles variables
UNRATE.d <- na.omit(diff(UNRATE$value))
DFF.d <- na.omit(diff(DFF$value))
GDPDEF.d <- na.omit((log(diff(GDPDEF$value))))
# test stationarité
adf_unrate_d <- tseries::adf.test(UNRATE.d)
adf_dff_d <- tseries::adf.test(DFF.d)
adf_gdpdef_d <- tseries::adf.test(GDPDEF.d)

adf_values_d <- data.frame(
  Série = c("UNRATE", "DFF", "GDPDEF"),
  `Statistique ADF` = c(adf_unrate_d$statistic, 
                        adf_dff_d$statistic, 
                        adf_gdpdef_d$statistic),
  `p-value` = c(adf_unrate_d$p.value, 
                adf_dff_d$p.value, 
                adf_gdpdef_d$p.value)
)
adf_values_d
```

---

# Graph de la série $I(2) \rightarrow I(1) \rightarrow I(0)$

```{r, fig.height=3}
tsplot(GDPDEF.d)
# GDPDEF.d n'est toujours pas stationnaire : on différencie une seconde fois
GDPDEF.dd <- na.omit(diff((log(diff(GDPDEF$value)))))
# test stationarité
tseries::adf.test(diff(GDPDEF.dd))
```

---

# Graph des séries stationnaires

```{r}
par(mfrow=c(3,1))
tsplot(UNRATE.d)
tsplot(DFF.d)
tsplot(GDPDEF.dd)
```

--- 

# Estimation VAR(1) -- UNRATE, FF, INF

```{r}
PolMon <- VAR(cbind(UNRATE.d, DFF.d, GDPDEF.dd), type = "none")

#summary(PolMon)
coef(PolMon)
```

---

# Modèle trivarié VAR(1) estimé

$$
UNRATE_t = 0.656^{\star\star\star} UNRATE_{t-1}   +   0.032 FF_{t-1}                   + 0.053 INF_{t-1}                    + \varepsilon_{UNRATE, t}
$$
$$
FF_t = -0.799^{\star\star\star} UNRATE_{t-1}      +   0.069 FF_{t-1}                   + 0.029 INF_{t-1}                    + \varepsilon_{FF, t}  
$$
$$
INF_t = -0.055 UNRATE_{t-1}                       +   0.004 FF_{t-1}                   - 0.479^{\star\star\star} INF_{t-1} + \varepsilon_{INF, t}
$$

*En pratique, pour vérifier la Taylor Rule on applique un VAR structurel (SVAR) afin d'imposer une relation **structurelle***

---

# Graph $\mathbf{y}_t, \widehat{\mathbf{y}}_t, \widehat{\boldsymbol{\varepsilon}}_t, ACF \text{ et } PACF \text{ de } \widehat{\boldsymbol{\varepsilon}}_t$

```{r, r, fig.height = 4.7, fig.show = 'asis', echo = FALSE, fig.keep = 'all'}
#lancer le plot de l'analyse var dans le rmd
par(mfrow = c(3, 1))
plot(PolMon)
```


```{r, echo = FALSE}
# # Standardisation des séries
# UNRATE.d.std <- scale(UNRATE.d)  # Moyenne 0, variance 1
# DFF.d.std <- scale(DFF.d)        # Moyenne 0, variance 1
# GDPDEF.dd.std <- scale(GDPDEF.dd)  # Moyenne 0, variance 1
# 
# # Modèle VAR avec données standardisées
# PolMon.std <- VAR(cbind(UNRATE.d.std[-1], DFF.d.std[-1], GDPDEF.dd.std), type = "none")
# summary(PolMon.std)
# coef(PolMon.std)
# plot(PolMon.std)
```

---

## VAR(P) est en réalité un VAR(1)

**Forme compagnon** :

\[ \mathbf{y}_t = \Phi_0 + \Phi_1 \mathbf{y}_{t-1} + \Phi_2 \mathbf{y}_{t-2} + \ldots + \Phi_P \mathbf{y}_{t-P} + \boldsymbol{\varepsilon}_t \]

**Reformuler en un seul VAR(1)** où \(\mu = E[\mathbf{y}_t] = (I - \Phi_1 - \ldots - \Phi_P)^{-1} \Phi_0\)

\[ \mathbf{z}_t = \Upsilon \mathbf{z}_{t-1} + \boldsymbol{\xi}_t \]

\[
\mathbf{z}_t =
\begin{pmatrix}
\mathbf{y}_t - \boldsymbol{\mu} \\
\mathbf{y}_{t-1} - \boldsymbol{\mu} \\
\vdots \\
\mathbf{y}_{t-P+1} - \boldsymbol{\mu}
\end{pmatrix}
, 
\Upsilon =
\begin{pmatrix}
\Phi_1 & \Phi_2 & \Phi_3 & \ldots & \Phi_{P-1} & \Phi_P \\
I_k & 0 & 0 & \ldots & 0 & 0 \\
0 & I_k & 0 & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & I_k & 0
\end{pmatrix}
\]

- Tous les résultats peuvent être directement appliqués à la forme compagnon.
- Peut également être utilisé pour transformer AR(P) en VAR(1)

---

## Revisiter les prévisions univariées

- Considérons l'AR(1) standard

$$
y_{t}=\phi_{0}+\phi_{1} y_{t-1}+\varepsilon_{t}
$$

- Prévision optimale à 1 étape :

$$
\begin{aligned}
\mathsf{E}_{t}\left[y_{t+1}\right] & =\mathsf{E}_{t}\left[\phi_{0}\right]+\mathsf{E}_{t}\left[\phi_{1} y_{t}\right]+\mathsf{E}_{t}\left[\varepsilon_{t+1}\right] \\
& =\phi_{0}+\phi_{1} y_{t}+0
\end{aligned}
$$

- Prévision optimale à 2 étapes :

$$
\begin{aligned}
\mathsf{E}_{t}\left[y_{t+2}\right] & =\mathsf{E}_{t}\left[\phi_{0}\right]+\mathsf{E}_{t}\left[\phi_{1} y_{t+1}\right]+\mathsf{E}_{t}\left[\varepsilon_{t+2}\right] \\
& =\phi_{0}+\phi_{1} \mathsf{E}_{t}\left[y_{t+1}\right]+0 \\
& =\phi_{0}+\phi_{1}\left(\phi_{0}+\phi_{1} y_{t}\right) \\
& =\phi_{0}+\phi_{1} \phi_{0}+\phi_{1}^{2} y_{t}
\end{aligned}
$$

- Prévision optimale à $h$ étapes :

$$
\mathsf{E}_{t}\left[y_{t+h}\right]=\sum_{i=0}^{h-1} \phi_{1}^{i} \phi_{0}+\phi_{1}^{h} y_{t}
$$

---

## Prévisions avec VAR

- Identique au cas univarié

$$
\mathbf{y}_{t}=\boldsymbol{\Phi}_{0}+\boldsymbol{\Phi}_{1} \mathbf{y}_{t-1}+\boldsymbol{\varepsilon}_{t}
$$

- Prévision optimale à 1 étape :

$$
\begin{aligned}
\mathsf{E}_{t}\left[\mathbf{y}_{t+1}\right] & =\mathsf{E}_{t}\left[\boldsymbol{\Phi}_{0}\right]+\mathsf{E}_{t}\left[\boldsymbol{\Phi}_{1} \mathbf{y}_{t}\right]+\mathsf{E}_{t}\left[\boldsymbol{\varepsilon}_{t+1}\right] \\
& =\boldsymbol{\Phi}_{0}+\boldsymbol{\Phi}_{1} \mathbf{y}_{t}+\mathbf{0}
\end{aligned}
$$

- Prévision optimale à $h$ étapes :

$$
\begin{aligned}
\mathsf{E}_{t}\left[y_{t+h}\right] & =\boldsymbol{\Phi}_{0}+\boldsymbol{\Phi}_{1} \boldsymbol{\Phi}_{0}+\ldots+\boldsymbol{\Phi}_{1}^{h-1} \boldsymbol{\Phi}_{0}+\boldsymbol{\Phi}_{1}^{h} \mathbf{y}_{t} \\
& =\sum_{i=0}^{h-1} \boldsymbol{\Phi}_{1}^{i} \boldsymbol{\Phi}_{0}+\boldsymbol{\Phi}_{1}^{h} \mathbf{y}_{t}
\end{aligned}
$$

- Prévision de plus haut ordre peut être calculée de manière récursive

$$
\mathsf{E}_{t}\left[\mathbf{y}_{t+h}\right]=\boldsymbol{\Phi}_{0}+\boldsymbol{\Phi}_{1} \mathsf{E}_{t}\left[\mathbf{y}_{t+h-1}\right]+\ldots+\boldsymbol{\Phi}_{P} \mathsf{E}_{t}\left[\mathbf{y}_{t+h-P}\right]
$$

---

## Qu'est-ce qui fait une bonne prévision ?

- Résidus de prévision

$$
\hat{e}_{t+h \mid t}=y_{t+h}-\hat{y}_{t+h \mid t}
$$

- Les résidus *ne sont pas* un bruit blanc
- Peut contenir une composante MA($h-1$)

  - Erreur de prévision pour $y_{t+1}-\widehat{y}_{t+1 \mid t-h+1}$ n'était pas connue au moment $t$.
  
- Tracez vos résidus
- ACF des résidus
- Régressions de Mincer-Zarnowitz

- Procédure à trois périodes

  - Échantillon d'entraînement : Utilisé pour construire le modèle
  - Échantillon de validation : Utilisé pour affiner le modèle
  - Échantillon d'évaluation : Test ultime, idéalement en une seule fois

---

## Prévision multi-étapes

**Deux méthodes**

- Méthode itérative

  - Construire un modèle pour les prévisions à 1 étape
$$
\mathbf{y}_{t}=\Phi_{0}+\Phi_{1} \mathbf{y}_{t-1}+\varepsilon_{t}
$$
  - Itérer la prévision jusqu'à la période $h$
$$
\hat{\mathbf{y}}_{t+h \mid t}=\sum_{i=0}^{h-1} \Phi_{1}^{i} \Phi_{0}+\Phi_{1}^{h} \mathbf{y}_{t}
$$
  - Utilise efficacement les informations
  - Impose beaucoup de structure au problème

- Méthode directe

  - Construire un modèle pour les prévisions à $h$ étapes
$$
\mathbf{y}_{t}=\Phi_{0}+\Phi_{h} \mathbf{y}_{t-h}+\boldsymbol{\varepsilon}_{t}
$$
  - Prévision directe en utilisant une méthode pseudo à 1 étape
$$
\hat{\mathbf{y}}_{t+h \mid t}=\Phi_{0}+\Phi_{h} \mathbf{y}_{t}
$$
  - Robuste à certaines non-linéarités

---

## Évaluation des prévisions multi-étapes

- L'évaluation des prévisions multi-étapes est identique à l'évaluation des prévisions à une étape avec une réserve
- Les erreurs de prévision à $h$ étapes peuvent être corrélées avec toute erreur de prévision non connue au moment $t$

$$
\hat{e}_{t+1 \mid t-h+1}, \hat{e}_{t+2 \mid t-h+2}, \ldots, \hat{e}_{t+h-1 \mid t-1}
$$

- Conduit à une structure MA($h-1$) dans les erreurs de prévision
- Solutions :

  - Utiliser une régression GMZ régulière avec un estimateur de covariance Newey-West
$$
y_{t+h}-\hat{y}_{t+h \mid t}=\beta_{1}+\beta_{2} \hat{y}_{t+h \mid t}+\gamma \mathbf{x}_{t}+\eta_{t}
$$
$H_{0}: \beta_{1}=\beta_{2}=\gamma=0, H_{1}: \beta_{1} \neq 0 \cup \beta_{2} \neq 0 \cup \gamma_{j} \neq 0 \exists j$
  - Modéliser explicitement le MA($h-1$) et utiliser un estimateur de covariance standard
$$
y_{t+h}-\hat{y}_{t+h \mid t}=\beta_{1}+\beta_{2} \hat{y}_{t+h \mid t}+\gamma \mathbf{x}_{t}+\eta_{t}+\sum_{i=1}^{h-1} \theta_{i} \eta_{t-i}
$$
  Note : Nulle est la même ; n'impose pas de restriction sur $\theta$

---

## Exemple : VAR de Politique Monétaire

- Prévisions produites de manière itérative pour 1 à 8 trimestres à l'avance
- Benchmark (*bm*) de marche aléatoire (*Fixed Forecast*) ou de moyenne constante
- AR et VAR sélectionnent la longueur de décalage en utilisant BIC
- Forcer la réversion à la moyenne dans l'échantillon en utilisant un estimateur à 2 étapes

  1. Estimer la moyenne de l'échantillon, et soustraire pour produire $\tilde{\mathbf{y}}_{t}=\mathbf{y}_{t}-\hat{\boldsymbol{\mu}}$
  2. Estimer VAR *sans* constante
  $$
  \tilde{\mathbf{y}}_{t}=\boldsymbol{\Phi}_{1} \tilde{\mathbf{y}}_{t-1}+\ldots+\boldsymbol{\Phi}_{P} \tilde{\mathbf{y}}_{t-P}+\boldsymbol{\varepsilon}_{t}
  $$
  3. Prévision puis ajouter la moyenne dans l'échantillon
  $$
  \mathsf{E}_{t}\left[\tilde{\mathbf{y}}_{t+h}\right]+\hat{\boldsymbol{\mu}}
  $$

- Évaluation basée sur MSE relatif :

$$
\text { Rel. } \mathrm{MSE}=\frac{\mathrm{MSE}}{\mathrm{MSE}_{b m}}, \mathrm{MSE}=1 / (T-h-R) \sum_{t=R}^{T-h}\left(y_{t+h}-\hat{y}_{t+h \mid t}\right)^{2}
$$

---

## Exemple : VAR de Politique Monétaire
\small
 | Horizon | Série | VAR |  | AR |  |
 | :--: | :--: | :--: | :--: | :--: | :--: |
 |  |  | Restreinte | Non restreinte | Restreinte | Non restreinte |
 | 1 | Chômage | 0.522 | 0.520 | 0.507 | 0.507 |
 |  | Taux des fonds fédéraux | 0.887 | 0.903 | 0.923 | 0.933 |
 |  | Inflation | 0.869 | 0.868 | 0.839 | 0.840 |
 | 2 | Chômage | 0.716 | 0.710 | 0.717 | 0.718 |
 |  | Taux des fonds fédéraux | 0.923 | 0.943 | 1.112 | 1.130 |
 |  | Inflation | 1.082 | 1.081 | 1.031 | 1.030 |
 | 4 | Chômage | 0.872 | 0.861 | 0.937 | 0.940 |
 |  | Taux des fonds fédéraux | 0.952 | 0.976 | 1.082 | 1.109 |
 |  | Inflation | 1.000 | 0.999 | 0.998 | 0.998 |
 | 8 | Chômage | 0.820 | 0.806 | 0.973 | 0.979 |
 |  | Taux des fonds fédéraux | 0.974 | 1.007 | 1.062 | 1.110 |
 |  | Inflation | 1.001 | 1.000 | 0.998 | 0.997 |
\normalsize

---

## Performances de prévision VAR vs AR

1. Le taux de chômage,  
2. Le taux des Fed Funds,  
3. L’inflation.  

Les prévisions sont réalisées à partir de **50 % de l’échantillon disponible**, et les paramètres du modèle sont réestimés à chaque itération. 

La précision des prévisions est mesurée par la **Mean Squared Error (MSE) hors échantillon**, comparée à celle d'un **modèle de référence** (random walk pour le Fed Funds rate et moyenne historique pour les autres variables). 

Deux variantes du VAR sont testées :  

  - **Modèle restreint** : Forcé à converger vers la moyenne historique.  
  - **Modèle non restreint** : Estime librement les paramètres, y compris l’intercept.  

Le VAR intègre des interactions entre plusieurs séries temporelles, offrant des prévisions plus précises que les modèles univariés dans 7 cas sur 12. Lorsqu’il n’est pas optimal, il reste compétitif. 

En **politique monétaire** et plus généralement en macroéconomie, permet d'anticiper l’impact des chocs et améliorer la prise de décision.

---

# Estimation

## Estimation et Identification

- Identification univariée : Box-Jenkins

  - Utiliser ACF et PACF pour déterminer l'ordre de décalage AR et MA
  - Examiner les résidus
  - Principe de parcimonie

- L'autocorrélation d'un processus scalaire est définie
$$
\rho_{s}=\frac{\gamma_{s}}{\gamma_{0}}
$$
où $\gamma_{s}$ est la $s^{\text {ème }}$ autocovariance
  - Coefficient de régression :
$$
y_{t}=\mu+\rho_{s} y_{t-s}+\varepsilon_{t}
$$

- Autocorrélation partielle $\psi_{s}$

  - Interprétation de régression de la $s^{\text {ème }}$ autocorrélation partielle :
$$
y_{t}=\mu+\phi_{1} y_{t-1}+\phi_{2} y_{t-2}+\ldots+\phi_{s-1} y_{t-s+1}+\psi_{s} y_{t-s}+\varepsilon_{t}
$$
  - $\psi$ est la $s^{\text {ème }}$ autocorrélation partielle

---

## CCF et PCCF 
### (Fonction Cross Correlation) et (Fonction Cross Correlation Partielle)

- Équivalents multivariés
  
  - ACF et PACF ont les mêmes définitions de régression
  - Fonction de corrélation croisée
  $$
  \begin{aligned}
  & \rho_{x y, s}=\frac{\mathsf{E}\left[\left(x_{t}-\mu_{x}\right)\left(y_{t-s}-\mu_{y}\right)\right]}{\sqrt{\mathrm{V}\left[x_{t}\right] \mathrm{V}\left[y_{t}\right]}}
  \end{aligned}
  $$
  $$
  \rho_{y x, s}=\frac{\mathsf{E}\left[\left(y_{t}-\mu_{y}\right)\left(x_{t-s}-\mu_{x}\right)\right]}{\sqrt{\mathrm{V}\left[x_{t}\right] \mathrm{V}\left[y_{t}\right]}}
  $$
  
  - Généralement différent
  - Fonction de corrélation partielle croisée $\psi_{x y, s}$
  $$
  \begin{aligned}
  x_{t}= & \phi_{0}+\phi_{x 1} x_{t-1}+\ldots+\phi_{x s-1} x_{t-(s-1)} \\
  & +\phi_{y 1} y_{t-1}+\ldots+\phi_{y s-1} y_{t-(s-1)}+\varphi_{x y, s} y_{t-s}+\varepsilon_{x, t}
  \end{aligned}
  $$
    - Peut aider à identifier l'ordre VAR

- Problème plus profond : trop nombreux et trop compliqués
- Solution simple : Sélection de modèle

---

## Interprétation des CCF et PCCF

- $y$ a une dynamique (Hétérogène) HAR et se "déverse" dans $x$

$$
\begin{aligned}
{\left[\begin{array}{c}
x_{t} \\
y_{t}
\end{array}\right]=} & {\left[\begin{array}{cc}
0.5 & 0.9 \\
.0 & 0.47
\end{array}\right]\left[\begin{array}{c}
x_{t-1} \\
y_{t-1}
\end{array}\right]+\sum_{i=2}^{5}\left[\begin{array}{cc}
0 & 0 \\
0 & 0.06
\end{array}\right]\left[\begin{array}{c}
x_{t-i} \\
y_{t-i}
\end{array}\right] } \\
& +\sum_{j=6}^{22}\left[\begin{array}{cc}
0 & 0 \\
0 & 0.01
\end{array}\right]\left[\begin{array}{c}
x_{t-j} \\
y_{t-j}
\end{array}\right]+\left[\begin{array}{c}
\varepsilon_{x, t} \\
\varepsilon_{y, t}
\end{array}\right]
\end{aligned}
$$
détaillée par 
$$
\begin{aligned}
\left[\begin{array}{c} x_{t} \\ y_{t} \end{array}\right] =
& \underbrace{\left[\begin{array}{cc} 0.5 & 0.9 \\ 0 & 0.47 \end{array}\right] \left[\begin{array}{c} x_{t-1} \\ y_{t-1} \end{array}\right]}_{\text{Dépendance immédiate sur } t-1} + \underbrace{\sum_{i=2}^{5} \left[\begin{array}{cc} 0 & 0 \\ 0 & 0.06 \end{array}\right] \left[\begin{array}{c} x_{t-i} \\ y_{t-i} \end{array}\right]}_{\text{Effet de mémoire moyen terme (lags 2-5) sur } y_t} \\
& + \underbrace{\sum_{j=6}^{22} \left[\begin{array}{cc} 0 & 0 \\ 0 & 0.01 \end{array}\right] \left[\begin{array}{c} x_{t-j} \\ y_{t-j} \end{array}\right]}_{\text{Effet de mémoire long terme (lags 6-22) sur } y_t}  + \underbrace{\left[\begin{array}{c} \varepsilon_{x, t} \\ \varepsilon_{y, t} \end{array}\right]}_{\text{Innovations}}
\end{aligned}
$$

--- 

## Graph des séries

- Données simulées

```{r, echo=FALSE}
simulate_VAR <- function(n = 1000, seed = 123) {
  set.seed(seed)  # Fixer la graine pour reproductibilité
  
  # Initialiser les vecteurs x et y
  x <- numeric(n + 22)  # Besoin de 22 lags pour démarrer
  y <- numeric(n + 22)
  
  # Coefficients des matrices
  A1 <- matrix(c(0.5, 0.9, 0, 0.47), nrow = 2, byrow = TRUE)  # Lag 1
  A2_5 <- matrix(c(0, 0, 0, 0.06), nrow = 2, byrow = TRUE)  # Lags 2 à 5
  A6_22 <- matrix(c(0, 0, 0, 0.01), nrow = 2, byrow = TRUE)  # Lags 6 à 22
  
  # Génération des erreurs aléatoires
  epsilon_x <- rnorm(n)
  epsilon_y <- rnorm(n)
  
  # Simulation
  for (t in 23:(n + 22)) {
    # Partie due au lag 1
    lag1 <- A1 %*% c(x[t-1], y[t-1])
    
    # Partie due aux lags 2 à 5
    lag2_5 <- rowSums(sapply(2:5, function(i) A2_5 %*% c(x[t-i], y[t-i])))
    
    # Partie due aux lags 6 à 22
    lag6_22 <- rowSums(sapply(6:22, function(i) A6_22 %*% c(x[t-i], y[t-i])))
    
    # Ajout du bruit
    x[t] <- lag1[1] + lag2_5[1] + lag6_22[1] + epsilon_x[t-22]
    y[t] <- lag1[2] + lag2_5[2] + lag6_22[2] + epsilon_y[t-22]
  }
  
  # Retourner uniquement les 1000 dernières valeurs (enlever la phase transitoire)
  data.frame(x = x[23:(n+22)], y = y[23:(n+22)])
}

# Exécuter la simulation
set.seed(42)
sim_data <- simulate_VAR()

# Tracer les séries simulées
plot(sim_data$x, type = "l", col = "blue", main = "Simulation du modèle VAR",
     ylab = "Valeurs", xlab = "Temps")
lines(sim_data$y, col = "red")
legend("topright", legend = c("x", "y"), col = c("blue", "red"), lty = 1)

```

---

## ACF et CCFS

```{r, fig.keep='all', echo=FALSE, fig.height=4}
par(mfrow = c(1, 2), mar = c(4, 4, 3, 3)) 
acf(sim_data$x, main = "ACF de x_t")
acf(sim_data$y, main = "ACF de y_t")
par(mfrow = c(1, 1))
ccf(sim_data$x, sim_data$y, main = "CCF entre x_t et y_t")
```

---

## Fonctions PACF

### PACF

```{r, echo=FALSE}
par(mfrow=c(2,1))
pacf(sim_data$x, main = "PACF de x_t")  
pacf(sim_data$y, main = "PACF de y_t")  
```

---

## Sélection de modèle

- Étape 1 : Choisir la longueur de décalage maximale
  - Critères d'information
  $$
  \begin{array}{ll}
  \text { AIC: } & \ln |\boldsymbol{\Sigma}(P)|+k^{2} P \frac{2}{T}\\
  \text { Hannan-Quinn IC (HQIC): } & \ln |\boldsymbol{\Sigma}(P)|+k^{2} P \frac{\ln \ln T}{T} \\
  \text { SIC: } & \ln |\boldsymbol{\Sigma}(P)|+k^{2} P \frac{\ln T}{T}
  \end{array}
  $$
    - $\Sigma(P)$ est la covariance des résidus en utilisant $P$ décalages
    - $|\cdot|$ est le déterminant
  - Basé sur l'hypothèse de test
    - Général à Spécifique
    - Spécifique à Général
  - Rapport de vraisemblance
  $$
  \left(T-P_{2} k^{2}\right)\left(\ln \left|\boldsymbol{\Sigma}\left(P_{1}\right)\right|-\ln \left|\boldsymbol{\Sigma}\left(P_{2}\right)\right|\right) \stackrel{A}{\sim} \chi_{\left(P_{2}-P_{1}\right) k^{2}}^{2}
  $$

---

## Sélection de la longueur de décalage dans le VAR de la Politique Monétaire

- Décalage maximal : 12 (1 an)

 | Longueur de décalage | AIC | HQIC | BIC | LR | P-val |
 | :--: | :--: | :--: | :--: | :--: | :--: |
 | 0 | 4.014 | 3.762 | 3.605 | 925 | 0.000 |
 | 1 | 0.279 | 0.079 | $0.000^{\nabla \Delta }$ | 39.6 | 0.000 |
 | 2 | 0.190 | 0.042 | 0.041 | 40.9 | 0.000 |
 | 3 | 0.096 | $0.000^{\nabla}$ | 0.076 | 29.0 | 0.001 |
 | 4 | $0.050^{\nabla}$ | 0.007 | 0.160 | 7.34 | $0.602^{\nabla}$ |
 | 5 | 0.094 | 0.103 | 0.333 | 29.5 | 0.001 |
 | 6 | 0.047 | 0.108 | 0.415 | 13.2 | 0.155 |
 | 7 | 0.067 | 0.180 | 0.564 | 32.4 | 0.000 |
 | 8 | 0.007 | $0.172^{\Delta}$ | 0.634 | 19.8 | 0.019 |
 | 9 | $0.000^{\Delta}$ | 0.217 | 0.756 | 7.68 | $0.566^{\Delta}$ |
 | 10 | 0.042 | 0.312 | 0.928 | 13.5 | 0.141 |
 | 11 | 0.061 | 0.382 | 1.076 | 13.5 | 0.141 |
 | 12 | 0.079 | 0.453 | 1.224 | - | - |

---

# Causalité de Granger

- Premier concept fondamentalement nouveau

- Examine si les décalages d'une variable sont utiles pour prédire une autre

## Définition (Causalité de Granger)

Une variable aléatoire scalaire $\left\{x_{t}\right\}$ est dite **ne pas** causer au sens de Granger $\left\{y_{t}\right\}$ si $\mathsf{E}\left[y_{t} \mid x_{t-1}, y_{t-1}, x_{t-2}, y_{t-2}, \ldots\right]=\mathsf{E}\left[y_{t} \mid y_{t-1}, y_{t-2}, \ldots\right]$. C'est-à-dire, $\left\{x_{t}\right\}$ ne cause pas au sens de Granger si la prévision de $y_{t}$ est la même que l'on conditionne ou non sur les valeurs passées de $x_{t}$.

---

## Causalité de Granger

- Se traduit directement en une restriction dans un VAR
- Non restreint

$$
\left[\begin{array}{l}
x_{t} \\
y_{t}
\end{array}\right]=\left[\begin{array}{l}
\phi_{01} \\
\phi_{02}
\end{array}\right]+\left[\begin{array}{ll}
\phi_{11} & \phi_{12} \\
\phi_{21} & \phi_{22}
\end{array}\right]\left[\begin{array}{l}
x_{t-1} \\
y_{t-1}
\end{array}\right]+\left[\begin{array}{l}
\varepsilon_{1, t} \\
\varepsilon_{2, t}
\end{array}\right]
$$

- Restreint de sorte que $x_{t}$ ne GC pas $y_{t}$

$$
\begin{gathered}
{\left[\begin{array}{l}
x_{t} \\
y_{t}
\end{array}\right]=\left[\begin{array}{l}
\phi_{01} \\
\phi_{02}
\end{array}\right]+\left[\begin{array}{ll}
\phi_{11} & \phi_{12} \\
0 & \phi_{22}
\end{array}\right]\left[\begin{array}{l}
x_{t-1} \\
y_{t-1}
\end{array}\right]+\left[\begin{array}{l}
\varepsilon_{1, t} \\
\varepsilon_{2, t}
\end{array}\right]} \\
x_{t}=\phi_{01}+\phi_{11} x_{t-1}+\phi_{12} y_{t-1}+\varepsilon_{1, t} \\
y_{t}=\phi_{02}+\phi_{22} y_{t-1}+\varepsilon_{2, t} \Leftarrow \text { Pas de } x_{t}!
\end{gathered}
$$

---

## Plus de causalité de Granger

- Dans le modèle de décalage $P$

$$
\mathbf{y}_{t}=\boldsymbol{\Phi}_{0}+\boldsymbol{\Phi}_{1} \mathbf{y}_{t-1}+\boldsymbol{\Phi}_{2} \mathbf{y}_{t-2}+\ldots+\boldsymbol{\Phi}_{P} \mathbf{y}_{t-P}+\boldsymbol{\varepsilon}_{t}
$$

l'hypothèse nulle est

$$
H_{0}: \phi_{i j, 1}=\phi_{i j, 2}=\ldots=\phi_{i j, P}=0
$$

- Alternative est

$$
H_{0}: \phi_{i j, 1} \neq 0 \text { ou } \phi_{i j, 2} \neq 0 \text { ou } \ldots \text { ou } \phi_{i j, P} \neq 0
$$

- Test du rapport de vraisemblance

$$
\left(T-P k^{2}\right)\left(\ln \left|\boldsymbol{\Sigma}_{r}\right|-\ln \left|\boldsymbol{\Sigma}_{u}\right|\right) \stackrel{A}{\sim} \chi_{P}^{2}
$$

- $\Sigma_{u}$ est la covariance des erreurs à partir du modèle non restreint
- $\Sigma_{r}$ est la covariance des erreurs à partir du modèle restreint
- $T-P k^{2}$ est le nombre d'observations moins le nombre de paramètres libres dans le modèle non restreint

  - Pourquoi $\chi_{P}^{2}$ ?

---

## VAR de Politique Monétaire -- Campbell

- Outil standard dans l'analyse de la politique monétaire
  - Taux de chômage (différencié)
  - Taux des fonds fédéraux
  - Taux d'inflation (différencié)

$$
\left[\begin{array}{c}
\Delta \mathrm{UNEMP}_{t} \\
\mathrm{FF}_{t} \\
\Delta \mathrm{INF}_{t}
\end{array}\right]=\Phi_{0}+\Phi_{1}\left[\begin{array}{c}
\Delta \mathrm{UNEMP}_{t-1} \\
\mathrm{FF}_{t-1} \\
\Delta \mathrm{INF}_{t-1}
\end{array}\right]+\left[\begin{array}{c}
\varepsilon_{1, t} \\
\varepsilon_{2, t} \\
\varepsilon_{3, t}
\end{array}\right]
$$

---

## Causalité de Granger dans le VAR de Campbell

- Utiliser le modèle avec 3 décalages (HQIC)
- $H_{0}: \phi_{i j, 1}=\phi_{i j, 2}=\phi_{i j, 3}=0$
- $H_{1}: \phi_{i j, 1} \neq 0$ ou $\phi_{i j, 2} \neq 0$ ou $\phi_{i j, 3} \neq 0$
- $i$ représente la série affectée par les décalages de la série $j$

\footnotesize
 |  | Taux des fonds fédéraux |  | Inflation |  | Chômage |  |
 | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
 | Exclusion | P-val | Stat | P-val | Stat | P-val | Stat |
 | Taux des fonds fédéraux | - | - | 0.001 | 13.068 | 0.014 | 8.560 |
 | Inflation | 0.001 | 14.756 | - | - | 0.375 | 1.963 |
 | Chômage | 0.000 | 19.586 | 0.775 | 0.509 | - | - |
 | Tous | 0.000 | 33.139 | 0.000 | 18.630 | 0.005 | 10.472 |
\normalsize

---

# Fonctions de réponse impulsionnelle

- Deuxième concept fondamentalement nouveau

- La dynamique complexe d'un VAR rend l'interprétation directe des coefficients difficile

- La solution est d'examiner les réponses impulsionnelles

- La fonction de réponse impulsionnelle de $y_{i}$ par rapport à un choc dans $\varepsilon_{j}$, pour tout $j$ et $i$, est définie comme le changement dans $y_{i t+s}, s \geq 0$ pour un choc unitaire dans $\varepsilon_{j t}$
  - Difficile à déchiffrer
  
- Tant que $\mathbf{y}_{t}$ est stationnaire en covariance, il doit avoir une représentation VMA,
$$
\mathbf{y}_{t}=\boldsymbol{\mu}+\boldsymbol{\varepsilon}_{t}+\boldsymbol{\Xi}_{1} \boldsymbol{\varepsilon}_{t-1}+\boldsymbol{\Xi}_{2} \boldsymbol{\varepsilon}_{t-2}+\ldots
$$

- $\boldsymbol{\Xi}_{j}$ sont les réponses impulsionnelles !

- Pourquoi ?
  - Mesurent directement l'effet dans la période $j$ de tout choc

---

## AP(P) et MA($\infty$)

- Tout $\operatorname{AR}(P)$ stationnaire
$$
y_{t}=\phi_{0}+\phi_{1} y_{t-1}+\phi_{2} y_{t-2}+\ldots+\phi_{P} y_{t-P}+\varepsilon_{t}
$$
peut être représenté comme un MA($\infty$)
$$
y_{t}=\phi_{0} /\left(1-\phi_{1}-\phi_{2}-\ldots-\phi_{P}\right)+\varepsilon_{t}+\sum_{i=1}^{\infty} \theta_{i} \varepsilon_{t-i}
$$
- $\operatorname{AR}(1)$
$$
y_{t}=\phi_{0}+\phi_{1} y_{t-1}+\varepsilon_{t}
$$
devient
$$
y_{t}=\phi_{0} /\left(1-\phi_{1}\right)+\varepsilon_{t}+\sum_{i=1}^{\infty} \phi_{1}^{i} \varepsilon_{t-i}
$$
- Les VAR(P) stationnaires ont la même relation avec $\operatorname{VMA}(\infty)$
$$
\begin{aligned}
\mathbf{y}_{t}&=\Phi_{0}+\Phi_{1} \mathbf{y}_{t-1}+\Phi_{2} \mathbf{y}_{t-2}+\ldots+\Phi_{P} \mathbf{y}_{t-P}+\boldsymbol{\varepsilon}_{t} \\
&\\
\mathbf{y}_{t}&=\boldsymbol{\mu}+\boldsymbol{\varepsilon}_{t}+\boldsymbol{\Xi}_{1} \boldsymbol{\varepsilon}_{t-1}+\boldsymbol{\Xi}_{2} \boldsymbol{\varepsilon}_{t-2}+\ldots
\end{aligned}
$$

---

## Résolution IR

- Facile dans VAR(1)

$$
y_{t}=\left(\mathbf{I}_{K}-\Phi_{1}\right)^{-1} \Phi_{0}+\varepsilon_{t}+\Phi_{1} \varepsilon_{t-1}+\Phi_{1}^{2} \varepsilon_{t-2}+\ldots
$$

- $\Xi_{j}=\Phi_{1}^{j}$
- Dans le VAR(P) général,

$$
\Xi_{j}=\Phi_{1} \Xi_{j-1}+\Phi_{2} \Xi_{j-2}+\ldots+\Phi_{P} \Xi_{j-P}
$$
où $\Xi_{0}=\mathbf{I}_{k}$ et $\Xi_{m}=0$ pour $m<0$.

  - Dans un VAR(2),
$$
\begin{gathered}
y_{t}=\Phi_{1} y_{t-1}+\Phi_{2} y_{t-2}+\varepsilon_{t} \\
-\Xi_{0}=\mathbf{I}_{k}, \Xi_{1}=\Phi_{1}, \Xi_{2}=\Phi_{1}^{2}+\Phi_{2}, \text { et } \Xi_{3}=\Phi_{1}^{3}+\Phi_{1} \Phi_{2}+\Phi_{2} \Phi_{1}
\end{gathered}
$$

- Les intervalles de confiance sont également assez compliqués

---

## Considérations pour les chocs

- VAR bivariée simple d'ordre 1

$$
\left[\begin{array}{l}
x_{t} \\
y_{t}
\end{array}\right]=\left[\begin{array}{l}
\phi_{01} \\
\phi_{02}
\end{array}\right]+\left[\begin{array}{ll}
\phi_{11} & \phi_{12} \\
\phi_{21} & \phi_{22}
\end{array}\right]\left[\begin{array}{l}
x_{t-1} \\
y_{t-1}
\end{array}\right]+\left[\begin{array}{l}
\varepsilon_{1, t} \\
\varepsilon_{2, t}
\end{array}\right]
$$

- La manière dont vous choquez importe
- Dépend de la corrélation entre $\varepsilon_{1, t}$ et $\varepsilon_{2, t}$
- 3 méthodes

  - Ignorer la corrélation et choquer simplement $\varepsilon_{j, t}$ avec un choc d'écart-type unitaire
  - Utiliser Cholesky pour factoriser $\Sigma$ et utiliser $\Sigma^{1 / 2} \mathbf{e}_{j}$ où $\mathbf{e}_{j}$ est un vecteur de zéros avec 1 dans la $j^{\text {ème }}$ position
  $$
  \boldsymbol{\Sigma}=\left[\begin{array}{cc}
  1 & .5 \\
  .5 & 1
  \end{array}\right] \quad \boldsymbol{\Sigma}_{C}^{1 / 2}=\left[\begin{array}{cc}
  1 & 0 \\
  .5 & .866
  \end{array}\right]
  $$
    - L'ordre des variables importe
    
  - "Réponse impulsionnelle généralisée" qui utilise une méthode de projection

---

## Exemple des différents chocs

- Définir la covariance des erreurs

$$
\boldsymbol{\Sigma}=\left[\begin{array}{cc}
\sigma_{x}^{2} & \sigma_{x} \sigma_{y} \rho \\
\sigma_{x} \sigma_{y} \rho & \sigma_{y}^{2}
\end{array}\right]
$$

- Standardisé

$$
\left[\begin{array}{c}
\sigma_{x} \\
0
\end{array}\right] \text { et }\left[\begin{array}{c}
0 \\
\sigma_{y}
\end{array}\right]
$$

- Cholesky

$$
\begin{gathered}
\boldsymbol{\Sigma}_{C}^{1 / 2}\left[\begin{array}{l}
0 \\
1
\end{array}\right]=\left[\begin{array}{cc}
\sigma_{x} & 0 \\
\sigma_{y} \rho & \sigma_{y} \sqrt{1-\rho^{2}}
\end{array}\right]\left[\begin{array}{l}
0 \\
1
\end{array}\right]=\left[\begin{array}{c}
0 \\
\sigma_{y} \sqrt{1-\rho^{2}}
\end{array}\right] \\
{\left[\begin{array}{cc}
\sigma_{x} & 0 \\
\sigma_{y} \rho & \sigma_{y} \sqrt{1-\rho^{2}}
\end{array}\right]\left[\begin{array}{l}
1 \\
0
\end{array}\right]=\left[\begin{array}{c}
\sigma_{x} \\
\sigma_{y} \rho
\end{array}\right], \text { autre est }\left[\begin{array}{c}
0 \\
\sigma_{y} \sqrt{1-\rho^{2}}
\end{array}\right]}
\end{gathered}
$$

---

## Réponses impulsionnelles

- Taux des fonds fédéraux ordonné en premier
- Réponse à un choc des fonds fédéraux
- Factorisation de Cholesky

```{r IRF_FF, fig.keep ='all', fig.show='hold', out.width='33%'}
irf_var_FF_INF <- irf(PolMon, impulse = "DFF.d", response = "GDPDEF.dd")
irf_var_FF_UNRATE <- irf(PolMon, impulse = "DFF.d", response = "UNRATE.d")
irf_var_FF_FF <- irf(PolMon, impulse = "DFF.d", response = "DFF.d")
par(mfrow=c(1,3))
plot(irf_var_FF_INF)
plot(irf_var_FF_UNRATE)
plot(irf_var_FF_FF)
par(mfrow=c(1,1))
```

---

## Cointégration

- La cointégration est la version VAR des racines unitaires
- Établit des relations à long terme entre deux variables à racine unitaire
  - La consommation a une racine unitaire, le revenu a une racine unitaire
  - Consommation - Revenu : ????

## Définition (Intégré d'Ordre 1)

Une variable $y_{t}$ est intégrée d'ordre 1, ou $\mathsf{I}(1)$, si $y_{t}$ est non stationnaire et $\Delta y_{t}=y_{t}-y_{t-1}$ est stationnaire.

---

## Cointégration

## Définition (Cointégration Bivariée)

Si $x_{t}$ et $y_{t}$ sont cointégrés si les deux sont I(1) et il existe un vecteur $\beta$ avec les deux éléments non nuls tels que

$$
\beta_{1} x_{t}-\beta_{2} y_{t} \sim I(0)
$$

- Lien fort entre $x_{t}$ et $y_{t}$
- Les deux sont des marches aléatoires mais la différence est à retour moyen
- Retour moyen à la tendance (tendance stochastique)

---

## À quoi ressemble la cointégration ?

$$\mathbf{y}_{t}=\boldsymbol{\Phi}_{i j} \mathbf{y}_{t-1}+\boldsymbol{\varepsilon}_{t} $$

$$
\boldsymbol{\Phi}_{11} =
\begin{bmatrix}
0.8 & 0.2 \\
0.2 & 0.8
\end{bmatrix}, \quad \lambda_i = 1, 0.6
$$

$$
\boldsymbol{\Phi}_{21} =
\begin{bmatrix}
0.7 & 0.2 \\
0.2 & 0.7
\end{bmatrix}, \quad \lambda_i = 0.9, 0.5
$$

$$
\boldsymbol{\Phi}_{12} =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}, \quad \lambda_i = 1, 1
$$

$$
\boldsymbol{\Phi}_{22} =
\begin{bmatrix}
-0.3 & 0.3 \\
0.1 & -0.2
\end{bmatrix}, \quad \lambda_i = -0.43, -0.06
$$
---

# Persistance, Anti-persistance et Cointégration

```{r cointegration, echo=FALSE}
simulate_VAR1 <- function(Phi, nobs = 100, Sigma = diag(2)) {
  k <- nrow(Phi)    
  Y <- matrix(0, nrow = nobs, ncol = k) 
  epsilon <- mvrnorm(nobs, mu = rep(0, k), Sigma = Sigma) # Bruit gaussien

  for (t in 2:nobs) {
    Y[t, ] <- Phi %*% Y[t-1, ] + epsilon[t, ]
  }
  return(as.data.frame(Y))
}

# Définition des matrices \Phi des slides
Phi_11 <- matrix(c(0.8, 0.2, 0.2, 0.8), nrow = 2, byrow = TRUE)
Phi_21 <- matrix(c(0.7, 0.2, 0.2, 0.7), nrow = 2, byrow = TRUE)
Phi_12 <- matrix(c(1, 0, 0, 1), nrow = 2, byrow = TRUE)
Phi_22 <- matrix(c(-0.3, 0.3, 0.1, -0.2), nrow = 2, byrow = TRUE)

# Simulation des séries temporelles
set.seed(1212)
nobs <- 100
data_11 <- simulate_VAR1(Phi_11, nobs)
data_21 <- simulate_VAR1(Phi_21, nobs)
data_12 <- simulate_VAR1(Phi_12, nobs)
data_22 <- simulate_VAR1(Phi_22, nobs)
 
colnames(data_11) <- colnames(data_21) <- colnames(data_12) <- colnames(data_22) <- c("y1", "y2")

# Graph
plot_VAR1 <- function(data, phi_label) {
  data$Time <- 1:nrow(data)
  ggplot(data, aes(x = Time)) +
    geom_line(aes(y = y1, color = "y1")) +
    geom_line(aes(y = y2, color = "y2")) +
    labs(
      title = bquote( ~ .(phi_label)),
      x = "Temps", 
      y = "Valeur", 
      color = NULL # Supprime le titre de la légende
    ) +
    scale_color_manual(values = c("y1" = "darkblue", "y2" = "darkred"),
                       labels = c(expression(y[1]), expression(y[2]))) + # Labels sans couleur
    theme_minimal() +
    theme(
      plot.title = element_text(size = 10) # Réduction de la taille du titre
    )
}
p1 <- plot_VAR1(data_11, bquote("Cointegration (" * Phi[11] * ")"))
p2 <- plot_VAR1(data_21, bquote("Persistant, Stationnaire (" * Phi[21] * ")"))
p3 <- plot_VAR1(data_12, bquote("Racines unitaires indépendantes (" * Phi[12] * ")"))
p4 <- plot_VAR1(data_22, bquote("Antipersistant, Stationnaire avec (" * Phi[22] * ")"))


grid.arrange(p1, p2, p3, p4, ncol = 2)

```

---

## Comment savons-nous quand un VAR est cointégré ?

- La condition des valeurs propres détermine si un VAR(1) est cointégré

$$
\left[\begin{array}{c}
y_{t} \\
x_{t}
\end{array}\right]=\left[\begin{array}{cc}
.8 & .2 \\
.2 & .8
\end{array}\right]\left[\begin{array}{c}
y_{t-1} \\
x_{t-1}
\end{array}\right]+\left[\begin{array}{c}
\varepsilon_{1, t} \\
\varepsilon_{2, t}
\end{array}\right]
$$

- Cointégré si une seule valeur propre est unitaire.
- Si tous inférieurs à 1 : ??
- Si les deux 1 : deux racines unitaires indépendantes

$$
\begin{aligned}
& \boldsymbol{\Phi}_{11}=\left[\begin{array}{cc}
.8 & .2 \\
.2 & .8
\end{array}\right] \quad \boldsymbol{\Phi}_{12}=\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right] \\
& \lambda_{i}=1,0.6 
& \lambda_{i}=1,1 \\
& \boldsymbol{\Phi}_{21}=\left[\begin{array}{cc}
.7 & .2 \\
.2 & .7
\end{array}\right] \quad \boldsymbol{\Phi}_{22}=\left[\begin{array}{cc}
-.3 & .3 \\
.1 & -.2
\end{array}\right] \\
& \lambda_{i}=0.9,0.5 
& \lambda_{i}=-0.43,-0.06
\end{aligned}
$$

---

# Modèles de Correction d'Erreur

- Point majeur de la cointégration
  - Cointégré $\Leftrightarrow$ Modèle de correction d'erreur

- Qu'est-ce qu'un modèle de correction d'erreur ?

  - VAR cointégré :
$$
\left[\begin{array}{l}
y_{t} \\
x_{t}
\end{array}\right]=\left[\begin{array}{ll}
.8 & .2 \\
.2 & .8
\end{array}\right]\left[\begin{array}{c}
y_{t-1} \\
x_{t-1}
\end{array}\right]+\left[\begin{array}{c}
\varepsilon_{1, t} \\
\varepsilon_{2, t}
\end{array}\right]
$$
  - Modèle de correction d'erreur :
$$
\left[\begin{array}{c}
\Delta y_{t} \\
\Delta x_{t}
\end{array}\right]=\left[\begin{array}{cc}
-.2 & .2 \\
.2 & -.2
\end{array}\right]\left[\begin{array}{c}
y_{t-1} \\
x_{t-1}
\end{array}\right]+\left[\begin{array}{c}
\varepsilon_{1, t} \\
\varepsilon_{2, t}
\end{array}\right]
$$
  - Forme normalisée
$$
\left[\begin{array}{c}
\Delta y_{t} \\
\Delta x_{t}
\end{array}\right]=\left[\begin{array}{c}
-.2 \\
.2
\end{array}\right]\left[\begin{array}{ll}
1 & -1
\end{array}\right]\left[\begin{array}{c}
y_{t-1} \\
x_{t-1}
\end{array}\right]+\left[\begin{array}{c}
\varepsilon_{1, t} \\
\varepsilon_{2, t}
\end{array}\right]
$$

- $[1 \quad - 1]$ est le vecteur de cointégration
- $[-.2 \quad .2]^{\prime}$ mesure la vitesse d'ajustement

---

## De VAR à VECM

$$
\left[\begin{array}{c}
y_{t} \\
x_{t}
\end{array}\right]=\left[\begin{array}{ll}
.8 & .2 \\
.2 & .8
\end{array}\right]\left[\begin{array}{c}
y_{t-1} \\
x_{t-1}
\end{array}\right]+\left[\begin{array}{c}
\varepsilon_{1, t} \\
\varepsilon_{2, t}
\end{array}\right]
$$

Soustraire $\left[y_{t-1} x_{t-1}\right]^{\prime}$ des deux côtés

$$
\begin{aligned}
& {\left[\begin{array}{c}
y_{t} \\
x_{t}
\end{array}\right]-\left[\begin{array}{c}
y_{t-1} \\
x_{t-1}
\end{array}\right]=\left[\begin{array}{ll}
.8 & .2 \\
.2 & .8
\end{array}\right]\left[\begin{array}{c}
y_{t-1} \\
x_{t-1}
\end{array}\right]-\left[\begin{array}{c}
y_{t-1} \\
x_{t-1}
\end{array}\right]+\left[\begin{array}{c}
\varepsilon_{1, t} \\
\varepsilon_{2, t}
\end{array}\right]} \\
& {\left[\begin{array}{c}
\Delta y_{t} \\
\Delta x_{t}
\end{array}\right]=\left(\left[\begin{array}{ll}
.8 & .2 \\
.2 & .8
\end{array}\right]-\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right]\right)\left[\begin{array}{c}
y_{t-1} \\
x_{t-1}
\end{array}\right]+\left[\begin{array}{c}
\varepsilon_{1, t} \\
\varepsilon_{2, t}
\end{array}\right]} \\
& {\left[\begin{array}{c}
\Delta y_{t} \\
\Delta x_{t}
\end{array}\right]=\left[\begin{array}{cc}
-.2 & .2 \\
.2 & -.2
\end{array}\right]\left[\begin{array}{c}
y_{t-1} \\
x_{t-1}
\end{array}\right]+\left[\begin{array}{c}
\varepsilon_{1, t} \\
\varepsilon_{2, t}
\end{array}\right]} \\
& {\left[\begin{array}{c}
\Delta y_{t} \\
\Delta x_{t}
\end{array}\right]=\left[\begin{array}{c}
-.2 \\
.2
\end{array}\right]\left[\begin{array}{ll}
1 & -1
\end{array}\right]\left[\begin{array}{c}
y_{t-1} \\
x_{t-1}
\end{array}\right]+\left[\begin{array}{c}
\varepsilon_{1, t} \\
\varepsilon_{2, t}
\end{array}\right]}
\end{aligned}
$$

---

## Vecteurs de cointégration

- La relation de cointégration peut toujours être décomposée

$$
\begin{aligned}
\Delta \mathbf{y}_{t} & =\boldsymbol{\pi} \mathbf{y}_{t-1}+\boldsymbol{\varepsilon}_{t} \\
\boldsymbol{\pi}      &=\boldsymbol{\alpha} \boldsymbol{\beta}^{\prime}
\end{aligned}
$$

- $\alpha$ mesure la vitesse de convergence
- $\beta$ contiennent les vecteurs de cointégration
- Le nombre de vecteurs de cointégration est $\operatorname{rank}\left(\boldsymbol{\alpha} \boldsymbol{\beta}^{\prime}\right)$

$$
\boldsymbol{\alpha} \boldsymbol{\beta}^{\prime}=\left[\begin{array}{cccc}
0.3 & 0.2 & -0.36 \\
0.2 & 0.5 & -0.35 \\
-0.3 & -0.3 & 0.39
\end{array}\right]
$$

- Combien ?

---

## Détermination des vecteurs de cointégration

$$
\begin{gathered}
\Delta \mathbf{y}_{t}=\boldsymbol{\pi} \mathbf{y}_{t-1}+\boldsymbol{\varepsilon}_{t} \\
\boldsymbol{\pi}=\left[\begin{array}{ccc}
0.3 & 0.2 & -0.36 \\
0.2 & 0.5 & -0.35 \\
-0.3 & -0.3 & 0.39
\end{array}\right]
\end{gathered}
$$

- Mettre $\boldsymbol{\pi}$ sous forme échelonnée par ligne

$$
\text { Forme Échelonnée par Ligne }=\left[\begin{array}{ccc}
1 & 0 & -1 \\
0 & 1 & -0.3 \\
0 & 0 & 0
\end{array}\right]
$$

- Rappel $\boldsymbol{\pi} =\boldsymbol{\alpha} \boldsymbol{\beta}^{\prime}$

$$
\boldsymbol{\beta}=\left[\begin{array}{rr}
1 & 0 \\
0 & 1 \\
-1 & -.3
\end{array}\right] \quad\quad \boldsymbol{\alpha}=\left[\begin{array}{rr}
.3 & .2 \\
.2 & .5 \\
-.3 & -.3
\end{array}\right]
$$

---

## Résolution des vecteurs de cointégration

$$
\begin{gathered}
\boldsymbol{\alpha} \boldsymbol{\beta}^{\prime}=\left[\begin{array}{ccc}
0.3 & 0.2 & -0.36 \\
0.2 & 0.5 & -0.35 \\
-0.3 & -0.3 & 0.39
\end{array}\right] \\
\text { Forme Échelonnée par Ligne } \Rightarrow\left[\begin{array}{ccc}
1 & 0 & -1 \\
0 & 1 & -0.3 \\
0 & 0 & 0
\end{array}\right] \\
\boldsymbol{\beta}=\left[\begin{array}{ll}
1 & 0 \\
0 & 1 \\
\beta_{1} & \beta_{2}
\end{array}\right]
\end{gathered}
$$

et $\boldsymbol{\alpha}$ a 6 paramètres inconnus. $\boldsymbol{\alpha} \boldsymbol{\beta}^{\prime}$ peut être combiné pour produire

$$
\boldsymbol{\pi}=\left[\begin{array}{lll}
\alpha_{11} & \alpha_{12} & \alpha_{11} \beta_{1}+\alpha_{12} \beta_{2} \\
\alpha_{21} & \alpha_{22} & \alpha_{21} \beta_{1}+\alpha_{22} \beta_{2} \\
\alpha_{31} & \alpha_{32} & \alpha_{31} \beta_{1}+\alpha_{32} \beta_{2}
\end{array}\right]
$$

---

## Tester la cointégration

- Deux tests pour la cointégration
  - Engle-Granger
  - Johansen
- Nous allons nous concentrer sur Engle-Granger
  - Simple et intuitif
  - Seulement applicable avec 1 relation de cointégration
- Tester la propriété clé de la cointégration : la différence est I(0)
- La plupart du travail est une simple OLS

$$
y_{t}=\delta_{0}+\beta x_{t}+\varepsilon_{t}
$$

- Reste du travail est de tester $\hat{\varepsilon}_{t}$ pour une racine unitaire
- Johansen teste les valeurs propres de $\pi=\alpha \beta^{\prime}$ directement.

---

# Procédure Engle-Granger

## Algorithme (Test Engle-Granger)

1. Commencer par analyser $x_{t}$ et $y_{t}$ isolément. Les deux doivent être des racines unitaires pour envisager la cointégration.
2. Estimer la relation à long terme
$$
y_{t}=\delta_{0}+\beta x_{t}+\varepsilon_{t}
$$
et tester $H_{0}: \gamma=0$ contre $H_{0}: \gamma<0$ dans la régression ADF

$$
\Delta \hat{\varepsilon}_{t}=\gamma \hat{\varepsilon}_{t-1}+\delta_{1} \Delta \hat{\varepsilon}_{t-1}+\ldots+\delta_{p} \Delta \hat{\varepsilon}_{t-P}+\eta_{t}
$$

3. En utilisant les paramètres estimés, spécifier et estimer la forme de correction d'erreur de la relation,

$$
\left[\begin{array}{c}
\Delta x_{t} \\
\Delta y_{t}
\end{array}\right]=\begin{array}{r}
\pi_{01} \\
\pi_{02}
\end{array}+\begin{array}{r}
\alpha_{1} \hat{\varepsilon}_{t} \\
\alpha_{2} \hat{\varepsilon}_{t}
\end{array}+\boldsymbol{\pi}_{1}\left[\begin{array}{c}
\Delta x_{t-1} \\
\Delta y_{t-1}
\end{array}\right]+\ldots+\boldsymbol{\pi}_{P}\left[\begin{array}{c}
\Delta x_{t-P} \\
\Delta y_{t-P}
\end{array}\right]+\left[\begin{array}{c}
\eta_{1, t} \\
\eta_{2, t}
\end{array}\right]
$$

4. Évaluer le modèle

---

## Considérations Engle-Granger

- Termes déterministes
  - Pas de termes déterministes : seulement dans des circonstances spéciales
  
  $$
  y_{t}=\beta x_{t}+\varepsilon_{t}
  $$
  
  - Constante : cas standard
  
  $$
  y_{t}=\delta_{0}+\beta x_{t}+\varepsilon_{t}
  $$
  
  - Constante et tendance temporelle : permettre différents taux de croissance/tendances temporelles dans les variables

  $$
  y_{t}=\delta_{0}+\delta_{1} t+\beta x_{t}+\varepsilon_{t}
  $$

## Valeurs Critiques

- Les valeurs critiques dépendent des déterministes dans la régression CI
  - Les modèles avec plus de déterministes ont des valeurs critiques plus basses (plus négatives)
- Les valeurs critiques dépendent du nombre de variables I(1) du côté droit
  - Les modèles plus grands ont des valeurs critiques plus basses

---

## Exemple : cay

- La relation consommation-richesse agrégée a été une série cointégrée intéressante dans la littérature financière récente
- A relancé le CCAPM
- Trois composantes :
  - Consommation (c)
  - Richesse en actifs (a)
  - Revenu du travail (richesse humaine) (y)
- Écart par rapport à la relation à long terme lié au rendement attendu
- Relation de cointégration : $c_{t}+0.643-0.249 a_{t}-0.785 y_{t}$

 |  | Tests de racine unitaire |  |  |
 | :--: | :--: | :--: | :--: |
 | Série | T-stat | P-val | Décalages ADF |
 | $c$ | $-1.198$ | 0.674 | 5 |
 | $a$ | $-0.205$ | 0.938 | 3 |
 | $y$ | $-2.302$ | 0.171 | 0 |
 | $\hat{\varepsilon}_{t}^{c}$ | $-2.706$ | 0.383 | 1 |
 | $\hat{\varepsilon}_{t}^{a}$ | $-2.573$ | 0.455 | 0 |
 | $\hat{\varepsilon}_{t}^{y}$ | $-2.679$ | 0.398 | 1 |

---

```{r}
# Télécharger les données
consommation <- fredr(series_id = "PCE", observation_start = as.Date("1955-01-01"), observation_end = as.Date("2019-12-31"))
richesse <- fredr(series_id = "TNWBSHNO", observation_start = as.Date("1955-01-01"), observation_end = as.Date("2019-12-31"))
revenu <- fredr(series_id = "PI", observation_start = as.Date("1955-01-01"), observation_end = as.Date("2019-12-31"))

# Fusionner les données
donnees <- consommation %>%
  select(date, valeur_c = value) %>%
  left_join(richesse %>% select(date, valeur_a = value), by = "date") %>%
  left_join(revenu %>% select(date, valeur_y = value), by = "date")

# Nettoyer les données
donnees <- na.omit(donnees)
donnees <- donnees %>%
  mutate(
    log_c = log(valeur_c),
    log_a = log(valeur_a),
    log_y = log(valeur_y)
  )
```


```{r}
# Test de stationnarité
adf.test(donnees$log_c)
adf.test(donnees$log_a)
adf.test(donnees$log_y)

# Test de cointégration de Johansen
jo_test <- ca.jo(donnees[, c("log_c", "log_a", "log_y")], type="eigen", K=2, spec="longrun")
summary(jo_test)

# johansen_test <- ca.jo(donnees[, c("log_c", "log_a", "log_y")], type = "trace", ecdet = "const", K = 2)
# summary(johansen_test)

# Estimer la relation de cointégration par régression
coint_reg <- lm(donnees$log_c ~ donnees$log_a + donnees$log_y, data = donnees)
residuals_coint <- residuals(coint_reg)
coeftest(coint_reg)

# Estimation du VECM
vecm <- cajorls(jo_test, r=1)
summary(vecm$rlm)

# Tests de diagnostic sur les résidus
vecm_residuals <- residuals(vecm$rlm)
serial.test(vecm_residuals)
arch.test(vecm_residuals)
normality.test(vecm_residuals)

# Visualisation des résultats
plot(jo_test)
plot(vecm_residuals)
```

```{r}
# Tracer la première série
plot(donnees$date, donnees$log_c, type = "l", col = "blue", lwd = 2,
     xlab = "Date", ylab = "Valeur", main = "Évolution des séries économiques")

# Ajouter les autres séries
lines(donnees$date, donnees$log_a, col = "red", lwd = 2, lty = "dashed")
lines(donnees$date, donnees$log_y, col = "green", lwd = 2, lty = "dotted")

# Ajouter une légende
legend("topleft", legend = c("Consommation (c)", "Richesse agrégée (a)", "Revenu du travail (y)"),
       col = c("blue", "red", "green"), lty = c("solid", "dashed", "dotted"), lwd = 2, bty = "n")
```




---

## Modèle Vectoriel de Correction d'Erreur

- VECM estimé en utilisant les résidus de la régression de cointégration

\small
$$
\left[\begin{array}{c}
\Delta c_{t} \\
\Delta a_{t} \\
\Delta y_{t}
\end{array}\right]=\left[\begin{array}{c}
0.003 \\
(0.000) \\
0.004 \\
(0.014) \\
0.003 \\
(0.000)
\end{array}\right]+\left[\begin{array}{c}
-0.000 \\
(0.281) \\
0.002 \\
(0.037) \\
0.000 \\
(0.515)
\end{array}\right] \hat{\varepsilon}_{t-1}+\left[\begin{array}{ccc}
0.192 & 0.102 & 0.139 \\
0.005) & (0.000) & (0.004) \\
0.282 & 0.220 & -0.149 \\
(0.116) & (0.006) & (0.414) \\
0.369 & 0.061 & -0.139 \\
(0.000) & (0.088) & (0.140)
\end{array}\right]\left[\begin{array}{c}
\Delta c_{t-1} \\
\Delta a_{t-1} \\
\Delta y_{t-1}
\end{array}\right]+\boldsymbol{\eta}_{t}
$$
\normalsize 

- Valeurs p entre parenthèses
- Estimation de la relation de cointégration n'a aucun effet sur les erreurs standard
  - Converge rapidement $(T)$
  - Les paramètres VECM convergent à la racine $\sqrt{T}$

---

## Régression Fallacieuse et Équilibre

- La prudence est nécessaire lorsqu'on travaille avec des données I(1)
  - I(0) sur I(0) : Le cas habituel. Les arguments asymptotiques standards s'appliquent.
  - I(1) sur I(0) : Cette régression est déséquilibrée.
  - I(1) sur I(1) : Cointégration ou régression fallacieuse.
  - I(0) sur I(1) : Cette régression est déséquilibrée.
- La régression fallacieuse peut conduire à de grandes statistiques t lorsque les séries sont indépendantes.
  - Deux processus I(1) non liés, $x_t$ et $y_t$ :
    $$
    x_t = x_{t-1} + \varepsilon_t
    $$
    $$
    y_t = y_{t-1} + \eta_t
    $$
  - Lorsque $T = 50$, environ 80% des statistiques $\mathsf{t}$ sont significatives
  - Toujours vérifier pour I(1) lors de l'utilisation de données de séries temporelles
  - Si les deux sont I(1), assurez-vous qu'ils sont cointégrés.


