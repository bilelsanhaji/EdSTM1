---
title: "Racines unitaires"
author: "Chapitre 3"
date: "Économétrie des séries temporelles"
output:
  beamer_presentation:
    number_sections: true
    toc: true
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{amssymb}
  - \setbeamertemplate{footline}{\hfill\raisebox{2pt}[0pt][0pt]\scriptsize{Économétrie des         Séries Temporelles - Chapitre 3}\hspace*{10pt} {\insertframenumber{} /                     \inserttotalframenumber}\hspace*{2pt}
    }
fontsize: 8pt
---

<!-- --- -->
<!-- title: "Racines unitaires" -->
<!-- author: "Chapitre 3" -->
<!-- date: "Économétrie des séries temporelles" -->
<!-- output: -->
<!--   xaringan::moon_reader: -->
<!--     css: [default, hygge-duke, hygge, custom_colors.css, custom_footer.css] -->
<!--     lib_dir: libs -->
<!--     nature: -->
<!--       ratio: "14:9" -->
<!--       highlightStyle: github -->
<!--       highlightLines: true -->
<!--       countIncrementalSlides: false -->
<!--       titleSlideClass: ["center", "middle", "my-title"] -->
<!--     mathjax: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" -->
<!-- --- -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  error = TRUE,
  fig.width = 10, 
  fig.height = 5,
  dpi = 600,
  fig.keep ="last",
  message = FALSE,
  warning = FALSE,
  results = "markup"
)
```

<!-- layout:true -->
<!-- <div class="footer"><span>Économétrie des Séries Temporelles - Chapitre 3</span></div> -->

<!-- comme \vspace{} -->
<!-- <div style="margin-top: -0.5em;"></div> -->

---

# Structure du cours

Marche aléatoire et marche aléatoire avec dérive

Processus stationnaires de tendance versus processus stationnaires de différence

Régression fallacieuse

Test de racine unitaire

Tests de Dickey-Fuller

Tests de Dickey-Fuller augmentés

Exemple empirique

Conclusion

---



De nombreuses séries temporelles macroéconomiques apparaissent non stationnaires lorsqu'elles sont analysées par des modèles AR :

$$y_{t} = \alpha y_{t-1} + \varepsilon_{t}, \quad \varepsilon_{t} \sim \mathsf{NID}(0, \sigma^{2})$$

**Marche aléatoire** ($\alpha = 1$) :

$$y_{t} = y_{0} + \sum_{j=1}^{t} \varepsilon_{j}$$

Persistance des chocs aléatoires – **mémoire infinie**

La marche aléatoire est courante dans les marchés financiers et des changes bilatéraux USD/GBP taux de change. 

**Marche aléatoire** – ne peut pas exploiter l'historique passé de $(y_{t}, \varepsilon_{t})$ pour systématiquement réaliser des profits en spéculant sur le taux de change futur.

Si $\alpha > 1$, $y_{t}$ est explosif.

---

```{r libs3, echo=FALSE}
library(fredr) #https://fred.stlouisfed.org/docs/api/api_key.html
fredr_set_key("229661c3a1cbd67e2388fafb768ca882")
library(astsa)
library(xts)
library(quantmod)
library(tseries)
library(lmtest)
library(urca)
```


## Marche aléatoire et processus explosif

```{r, echo=F}
par(mfrow=c(3,1))
x = arima.sim(list(order = c(1,0,0), ar = 0.95), n = 200)
tsplot(x, main = "AR(1), ar = 0.95")
x = arima.sim(list(order = c(1,0,0), ar = 0.9999), n = 200)
tsplot(x, main = "Marche Aléatoire, ar = 1")
# Définir les paramètres
set.seed(123)  # Pour la reproductibilité
T <- 200  # Nombre total de périodes
ar_coeff <- 1.2  # Coefficient AR

# Initialiser la série temporelle
yt <- numeric(T)
yt[1] <- rnorm(1)  # Valeur initiale

# Simuler la série temporelle avec un processus explosif
for (t in 2:T) {
  yt[t] <- ar_coeff * yt[t-1] + rnorm(1)
}

# Tracer la série temporelle
plot(yt, type = "l", col = "blue", 
main = " Process AR(1) explosif", ylab = "", xlab = "")
```

---

## Marche aléatoire avec dérive

$$y_{t} = \mu + y_{t-1} + \varepsilon_{t}, \quad \varepsilon_{t} \sim \mathsf{NID}(0, \sigma^{2})$$

$\mu$ est le terme de dérive.

Par substitution récursive :

$$y_{t} = y_{0} + \mu t + \sum_{j=1}^{t} \varepsilon_{j}$$

La tendance déterministe est représentée par $\mu t$.

$$\mathsf{E}(y_{t}) = y_{0} + \mu t$$
$$\mathsf{V}(y_{t}) = \gamma_{t}(0) = t \sigma^{2}$$
$$\text{cov}(y_{t}, y_{t-h}) = \gamma_{t}(h) = (t - h) \sigma^{2}$$

---

## Marche aléatoire avec dérive - graph

```{r, echo=F}
# Définir les paramètres
set.seed(123)  # Pour la reproductibilité
T <- 500  # Nombre total de périodes
y0 <- 0  # Valeur initiale
mu <- 0.4  # Coefficient de tendance

# Initialiser la série temporelle
yt <- numeric(T)
yt[1] <- y0  # Valeur initiale

# Simuler le processus
for (t in 2:T) {
  yt[t] <- yt[t-1] + mu + rnorm(1)
}

# Tracer la série temporelle
par(mfrow=c(2,1))
tsplot(yt, type = "l", col = "blue", main = "Process simulé", ylab = "", xlab = "")
tsplot(diff(yt), type = "l", col = "blue", main = "Process simulé différencié", ylab = "", xlab = "")
abline(h = mean(diff(yt)), col = "red", lty = 2, lwd = 2)
legend("topright", legend = "Moyenne", col = "red", lty = 2, lwd = 2, bty = "n")
```

---

## 1 Processus stationnaires de différence (DS) et de tendance (TS)

Modèle stationnaire de tendance :

$$y_{t} = \mu + \beta t + \varepsilon_{t}, \quad \varepsilon_{t} \sim \mathsf{NID}(0, \sigma^{2})$$

Tendance déterministe – rendue stationnaire en soustrayant la tendance.

Tendance exponentielle exhibant une croissance proportionnelle constante :

$$Y_{t} = e^{\delta t}$$

$$\frac{dY}{dt} = \delta Y_{t}$$

Les données sont généralement en logarithmes, $y = \log Y$, – donc tendance linéaire.

Il est souvent difficile de distinguer entre les modèles TS et DS.

Mais la persistance des chocs est différente entre TS et DS – chocs permanents versus chocs transitoires.

---

## Stationnaire en tendance

```{r, echo=F}
# Définir les paramètres
set.seed(123)  # Pour la reproductibilité
T <- 500  # Nombre total de périodes
y0 <- 0  # Valeur initiale
mu <- 0.4  # Coefficient de tendance

# Initialiser la série temporelle
yt <- numeric(T)
yt[1] <- y0  # Valeur initiale

# Simuler le processus
for (t in 2:T) {
  yt[t] <- y0 + mu * t + rnorm(t)
}

# Tracer la série temporelle
par(mfrow=c(2,1))
tsplot(yt, type = "l", col = "blue", main = "Stationnaire en tendance", ylab = "", xlab = "")
tsplot(diff(yt), type = "l", col = "blue", main = "Diff(Stationnaire en tendance)", ylab = "", xlab = "")
abline(h = mean(diff(yt)), col = "red", lty = 2, lwd = 2)
legend("topright", legend = "Moyenne", col = "red", lty = 2, lwd = 2, bty = "n")
```

---

## Types de modèles univariés

$$y_{t} = \mu + \alpha y_{t-1} + \beta t + \varepsilon_{t}, \quad \varepsilon_{t} \sim \mathsf{NID}(0, \sigma^{2})$$

| Paramètre | Description | Propriétés |
|-----------|-------------|------------|
| $\mu \neq 0; \mid\alpha\mid < 1; \beta \neq 0$ | Tendance déterministe avec composante AR(1) stationnaire | $\mathsf{I}(0)$ |
| $\mu \neq 0; \alpha = 1; \beta \neq 0$ | Marche aléatoire avec dérive et tendance quadratique | $\mathsf{I}(1)$ |
| $\mu \neq 0; \alpha = 1; \beta = 0$ | Marche aléatoire avec dérive | $\mathsf{I}(1)$ |
| $\mu \neq 0; \alpha = 0; \beta \neq 0$ | Tendance déterministe | $\mathsf{I}(0)$ |
| $\mu = 0; \alpha = 1; \beta = 0$ | Marche aléatoire pure | $\mathsf{I}(1)$ |

---

## Pour le cas marche aléatoire avec dérive et tendance quadratique

$$y_{t} = \mu + \alpha y_{t-1} + \beta t + \varepsilon_{t}$$

Donc :

$$\Delta y_{t} = \mu + \beta t + \varepsilon_{t} \sim \mathsf{I}(0)$$

Si $\Delta y_{t} \sim \mathsf{I}(0) \Rightarrow \sum_{i=1}^{t} [\Delta y_{i}] \sim \mathsf{I}(1)$


$$y_{t} = \mu t + \beta t^{2} + \sum_{i=1}^{t} \varepsilon_{i}$$

Donc, une tendance quadratique dans le niveau.

Peu probable de se produire en pratique, sauf localement.

---

<!-- ### Processus à racine unitaire

Le processus à racine unitaire est donné par :
$$\begin{aligned}
y_{t} &= \mu + y_{t-1} + \varepsilon_{t} \\
&= \mu t + y_{0} + \sum_{j=1}^{t} \varepsilon_{j}
\end{aligned}$$
où $\sum_{j=1}^{t} \varepsilon_{j}$ est une marche aléatoire ;
$y_{0}$ est la condition initiale ;
et $\mu t$ est le terme de dérive.

**Problème :** La théorie de la distribution pour les estimateurs et les statistiques de test diffère du cas transversal/stationnaire -- en raison de la dépendance temporelle. 
-->

## Régression fallacieuse

Deux séries non apparentées $\mathsf{I}(1)$.

Régression de $y$ sur $z$ $\Rightarrow$ relation apparemment significative.

Transformations pour induire la stationnarité :

  - Différenciation

  - Suppression de la tendance déterministe

  - Cointégration

---

### Rappel

Régression OLS standard pour des données stationnaires :
\begin{equation}
y_{t} = \beta x_{t} + u_{t}, \quad u_{t} \sim \mathsf{NID}(0, \sigma^{2})
\end{equation}

$\widehat{\beta} = \left[\sum_{t=1}^{T} x_{t}^{2}\right]^{-1} \sum_{t=1}^{T} x_{t} y_{t} = \left(X^{\prime} X\right)^{-1} \left(X^{\prime} y\right)$

$\mathsf{E}\left(\widehat{\beta} | x_{1}, \cdots, x_{T}\right) = \beta$

$\mathsf{V}\left(\widehat{\beta} | x_{1}, \cdots, x_{T}\right) = \sigma^{2} \left(X^{\prime} X\right)^{-1}$

---

### Résultats asymptotiques

\begin{equation}
\widehat{\beta} - \beta = \frac{\frac{1}{T} \sum_{t=1}^{T} x_{t} u_{t}}{\frac{1}{T} \sum_{t=1}^{T} x_{t}^{2}}
\end{equation}

En utilisant la loi faible des grands nombres :
$$\begin{aligned}
\frac{1}{T} \sum_{t=1}^{T} x_{t} u_{t} &\overset{p}{\longrightarrow} \mathsf{E}\left(x_{t} u_{t}\right) = 0 \nonumber \\
\frac{1}{T} \sum_{t=1}^{T} x_{t}^{2} &\overset{p}{\longrightarrow} \mathsf{E}\left(x_{t}^{2}\right) = \mathbf{Q} \neq 0
\end{aligned}$$

Donc :
\begin{equation}
\widehat{\beta} - \beta \overset{p}{\longrightarrow} \mathbf{Q}^{-1} \times 0 = 0
\end{equation}
ce qui établit la convergence.

---

### Résultats asymptotiques (suite)

\begin{equation}
\sqrt{T} \left(\widehat{\beta} - \beta\right) = \sqrt{T} \frac{\frac{1}{T} \sum_{t=1}^{T} x_{t} u_{t}}{\frac{1}{T} \sum_{t=1}^{T} x_{t}^{2}}
\end{equation}

Comme $\frac{1}{T} \sum_{t=1}^{T} x_{t}^{2} \overset{p}{\longrightarrow} \mathbf{Q}$ (une constante) et
$\frac{1}{\sqrt{T}} \sum_{t=1}^{T} x_{t} u_{t}$ a une distribution normale limite, alors
$\sqrt{T} \left(\widehat{\beta} - \beta\right)$ est asymptotiquement normalement distribué.

La preuve de la convergence nécessite $\mathsf{plim}\left(1/T\right) \left(X^{\prime} X\right) = \mathbf{Q}$ -- avec
l'augmentation de l'information de l'échantillon, les moments de l'échantillon convergent vers les moments de la population (fixes).

Par conséquent, les moments de la population doivent être stationnaires.

**Que se passe-t-il si les données ne sont pas stationnaires ?**

---

### Données non stationnaires -- régression fallacieuse

**DGP** : 

$$\begin{aligned}
y_{t} &= y_{t-1} + u_{t}  \\
x_{t} &= x_{t-1} + v_{t}  \\
\mathsf{E}(u_{t} v_{s}) &= 0, \forall t, s; \quad \mathsf{E}(u_{t} u_{k}) = \mathsf{E}(v_{t} v_{k}) = 0, \forall k \neq 0 \\
y_{0} &= x_{0} = 0 \nonumber
\end{aligned}$$

c'est-à-dire, $x_{t}$ et $y_{t}$ sont des marches aléatoires non corrélées.

Hypothèse économique :
\begin{equation}
y_{t} = \beta_{0} + \beta_{1} x_{t} + \varepsilon_{t}
\end{equation}

Nous devrions trouver $\beta_{1} = \frac{\partial y_{t}}{\partial x_{t}} \overset{p}{\longrightarrow} 0$.

---

Yule (1926) -- les corrélations fallacieuses peuvent persister dans de grands échantillons.

Sous $H_{0} : \beta_{1} = 0$, nous nous attendrions à ce que $\mathsf{E}(\widehat{\beta}_{1}) = 0$.

Pour des régresseurs non stationnaires, cela ne tient pas.

Les statistiques t conventionnelles sur $\widehat{\beta}_{0}$ et $\widehat{\beta}_{1}$ n'ont pas de distributions t.

Elles divergent en distribution à mesure que $T$ augmente. Par conséquent, pour $H_{0} : \beta_{1} = 0$, nous rejetterions plus fréquemment à mesure que $T$ augmente.

Problème d'équilibre : $y_{t} \sim {\mathsf I}(1)$ et $\varepsilon_{t}$ supposé $\sim {\mathsf I}(0)$.

Alors $y_{t} = \beta_{0} + \beta_{1} x_{t} + \varepsilon_{t}$ pourrait être bien défini pour $\beta_{1}$ non nul comme $x_{t} \sim {\mathsf I}(1)$.

Lorsque $\beta_{1} = 0$, $\{\varepsilon_{t}\}$ doit être ${\mathsf I}(1)$ -- ce qui viole les hypothèses.

---

Pour les données non stationnaires, $\widehat{\beta}_{1}$ ne converge pas vers $0$, mais converge plutôt vers une variable aléatoire.

$$\widehat{\beta}_{1} = \left[T^{-1} \sum_{t=1}^{T} \left(x_{t} - \overline{x}\right)^{2}\right]^{-1} T^{-2} \sum_{t=1}^{T} \left(x_{t} - \overline{x}\right) \left(y_{t} - \overline{y}\right)$$

Le numérateur et le dénominateur convergent faiblement vers des fonctionnelles du mouvement brownien.

Comme $x \sim {\mathsf I}(1)$, la variance de l'échantillon $= f(T)$.

Régression déséquilibrée, $\varepsilon \sim {\mathsf I}(1)$, la variance de l'échantillon $= f(T)$.

$\widehat{\beta}_{0}$ et $\widehat{\beta}_{1}$ minimisent $\widehat{\sigma}^{2}_{\varepsilon}$.

On observe que $\widehat{\sigma}_{\beta_{1}} = \widehat{\sigma}_{\varepsilon} \left(\sum_{t=1}^{T} \left(x_{t} - \overline{x}\right)^{2}\right)^{-\frac{1}{2}}$ est une fonction de $T$.

Par conséquent, ${\mathsf t} = \frac{\widehat{\beta}_{1}}{\widehat{\sigma}_{\beta_{1}}}$ diverge lorsque $T \rightarrow \infty$.

---

# Tests de racine unitaire

## Dickey-Fuller test

$$y_{t} = \alpha y_{t-1} + \varepsilon_{t}, \quad \varepsilon_{t} \sim \mathsf{IID}(0, \sigma^{2})$$

Définissons $\gamma = \alpha - 1$:

$$\Delta y_{t} = \gamma y_{t-1} + \varepsilon_{t}$$

Si racine unitaire, $\alpha = 1 \Rightarrow \gamma = 0$.

Par conséquent, nous souhaitons tester :

$$\mathsf{H_{0}} : \gamma = 0$$


Cela suggère OLS et le rejet de l'hypothèse nulle si $\widehat{\gamma}$ est significativement négatif.

**Problème** : il est nécessaire de connaître la distribution de la statistique de test sous l'hypothèse nulle.

Sous l'hypothèse nulle, la série est non stationnaire. Cela implique que la statistique de test n'est pas distribuée asymptotiquement selon une loi normale.

---

## Dickey-Fuller test (suite)

Considérons l'estimateur de $\widehat{\gamma}$ lorsque $\gamma = 0$ :

$$\widehat{\gamma} = \frac{\frac{1}{T} \sum_{t=1}^{T} y_{t-1} \varepsilon_{t}}{\frac{1}{T} \sum_{t=1}^{T} y_{t-1}^{2}}$$

En utilisant $y_{t-1} = \sum_{j=1}^{t-1} \varepsilon_{j}$ :

$$T \widehat{\gamma} = \frac{\frac{1}{T} \sum_{t=1}^{T} \left(\sum_{j=1}^{t-1} \varepsilon_{j}\right) \varepsilon_{t}}{\frac{1}{T^{2}} \sum_{t=1}^{T} \left(\sum_{j=1}^{t-1} \varepsilon_{j}\right)^{2}} \label{DF1}$$

$\sum_{j=1}^{t-1} \varepsilon_{j}$ n'est pas ergodique, ni $\mathsf{IID}$, donc la loi des grands nombres (LLN) ne s'applique pas.

$\sum_{j=1}^{t-1} \varepsilon_{j} \varepsilon_{t}$ n'est pas une séquence de différences martingales satisfaisant les hypothèses pour le théorème central limite (CLT).

---

## Dickey-Fuller test (suite)

Les distributions limites de $T \widehat{\gamma}$ ont été simulées par Dickey et Fuller (JASA, 1979) et MacKinnon (1991).

Ces distributions sont appelées distributions de Dickey-Fuller.

Les logiciels économétriques produisent des valeurs critiques à partir de surfaces de réponse.

Le test de racine unitaire permet de distinguer entre les données $\textsf{I}(0)$ et $\textsf{I}(1)$.

Mais différentes hypothèses nulles sont possibles en fonction des termes déterministes.

Par conséquent, différentes statistiques de test en fonction des hypothèses nulles.

---

Trois cas à considérer :

| Test       | Modèle                                      | Hypothèse                  |
|------------|--------------------------------------------|-----------------------------|
| $\widehat{\tau}$ | $\Delta y_{t} = \gamma y_{t-1} + \varepsilon_{t}$ | $H_{0} : \gamma = 0$ |
| $\widehat{\tau}_{\mu}$ | $\Delta y_{t} = \mu + \gamma y_{t-1} + \varepsilon_{t}$ | $H_{0} : \mu = 0; \gamma = 0$ |
| $\widehat{\tau}_{\beta}$ | $\Delta y_{t} = \mu + \gamma y_{t-1} + \beta t + \varepsilon_{t}$ | $H_{0} : \beta = 0; \gamma = 0$ |

Les valeurs critiques dépendent de la spécification des hypothèses nulles et alternatives.

Un test unilatéral est généralement utilisé pour maximiser la puissance :

$\mathsf{H}_{0} : \gamma = 0$ versus $\mathsf{H}_{1} : \gamma < 0$; car $\gamma > 0 \Rightarrow$ processus explosif.

| Distribution          | 2.5%  | 5%    | 10%   | 50% | 90%   | 95%   | 97.5% |
|--------------         |------ |-----  |------ |-----|------ |------ |-------|
| $\mathsf{N}(0,1)$     | -1.96 | -1.64 | -1.28 | 0   | 1.28  | 1.64  | 1.96 |
| $\mathsf{DF}_{\mu}$   | -3.12 | -2.86 | -2.57 |     | -0.44 | -0.07 | 0.23 |
| $\mathsf{DF}_{\beta}$ | -3.66 | -3.41 | -3.12 |     | -1.25 | -0.94 | -0.66 |

---

## Procédure de test

1. Évaluer les données graphiquement.

2. Si une tendance est probable, tester l'hypothèse conjointe $\mathsf{H}_{0} : \gamma = 0 ; \beta = 0$. La non-rejection $\Rightarrow$ processus DS sans tendance déterministe.

3. $\mathsf{H}_{0}$ pourrait être rejeté en raison de :

    - $\gamma \neq 0 ; \beta = 0$ : Aucun mécanisme générant une tendance.
    - $\gamma = 0 ; \beta \neq 0$ : Correspond à une tendance quadratique dans le DGP.
    - $\gamma \neq 0 ; \beta \neq 0$ : Tendance stationnaire ($\gamma < 0$).
4. Si les données ne présentent pas de tendance, tester l'hypothèse conjointe $\mathsf{H}_{0} : \gamma = 0 ; \mu = 0$.

5. $\mathsf{H}_{0}$ pourrait être rejeté en raison de :

    - $\gamma \neq 0 ; \mu = 0$ : Stationnarité.
    - $\gamma = 0 ; \mu \neq 0$ : Donnera une dérive notable.
    - $\gamma \neq 0 ; \mu \neq 0$ : Distribution normale standard asymptotique.

6. Si la moyenne est zéro, sans tendance, tester l'hypothèse $\mathsf{H}_{0} : \gamma = 0$.

---

# Augmented Dickey-Fuller Test

Le test est considéré sur la base d'un processus AR(1).
Si le DGP est un processus AR(p) :

$$y_{t} = \mu + \alpha_{1} y_{t-1} + \cdots + \alpha_{p} y_{t-p} + \varepsilon_{t}$$

mais un modèle AR(1) est ajusté :

$$y_{t} = \mu + \alpha_{1} y_{t-1} + \nu_{t}$$

alors :

$$\nu_{t} = \alpha_{2} y_{t-2} + \cdots + \alpha_{p} y_{t-p} + \varepsilon_{t}$$

ce qui génère une autocorrélation sérielle.

Ajuster un modèle AR(p) en utilisant une stratégie de recherche générale à spécifique.
Commencer avec un grand $p$ et tester vers le bas pour s'assurer qu'il n'y a pas d'autocorrélation sérielle.

---

Considérons un processus AR(2) :

$$\begin{aligned}
y_{t} &= \mu + \alpha_{1} y_{t-1} + \alpha_{2} y_{t-2} + \varepsilon_{t}  \\
&= \mu + \left(\alpha_{1} + \alpha_{2}\right) y_{t-1} - \alpha_{2} \left(y_{t-1} - y_{t-2}\right) + \varepsilon_{t}  \\
\Delta y_{t} &= \mu + \gamma y_{t-1} + \phi_{1} \Delta y_{t-1} + \varepsilon_{t}
\end{aligned}$$

où $\gamma = \alpha_{1} + \alpha_{2} - 1$ et $\phi_{1} = -\alpha_{2}$.

Le modèle de Dickey-Fuller est augmenté par $\Delta y_{t-1}$.

Tester $\mathsf{H}_{0} : \gamma = 0$ pour $\widehat{\tau}_{\mu}$ comme avant.

Modèle ADF(p) avec intercept et tendance :

$$\Delta y_{t} = \mu + \gamma y_{t-1} + \sum_{j=1}^{p-1} \phi_{j} \Delta y_{t-j} + \beta t + \varepsilon_{t}$$

<div style="margin-top: -1em;"></div>

où

<div style="margin-top: -1em;"></div>

$$\begin{aligned}
\gamma &= \alpha_{1} + \alpha_{2} + \cdots + \alpha_{p} - 1,  \\
\phi_{1} &= -\left(\alpha_{2} + \cdots + \alpha_{p}\right)  \\
& \;\; \vdots \\
\phi_{p-1} &= -\alpha_{p}
\end{aligned}$$

---

# Illustration empirique

## Données 

```{r}
USunrate = fredr(series_id = "UNRATE",
           observation_start = as.Date("1948-01-01"),
           observation_end = as.Date("2023-10-01"),
           frequency = "a")
x = USunrate$value
xDate = USunrate$date
```

---

## $USunrate$, $\Delta USunrate$, $\Delta^{2} USunrate$

<div style="margin-top: -1.5em;"></div>

```{r fig.width = 10, fig.height = 4.0}
par(mfrow=c(3,1))
tsplot(xDate,x, 
main="Taux de chomage", ylab = "%", xlab='')
# Si on différencie la série 1 fois :
tsplot(xDate[-1],diff(x),main= expression(Delta ~ "Taux de chomage aux USA"),
       ylab = "%", xlab='')
# Si on différencie la série 2 fois :
tsplot(xDate[-1][-1],diff(diff(x)),main= 
         expression(Delta^2 ~ "Taux de chomage aux USA"),ylab = "%",xlab='')
```

---

<div style="margin-top: -1.5em;"></div>

## Tester la stationarité

<div style="margin-top: -1.5em;"></div>

### ADF {tseries}

<div style="margin-top: -1.5em;"></div>

```{r}
adf.test(x)
adf.test(diff(x))
adf.test(diff(diff(x)))
```
---

<div style="margin-top: -1.5em;"></div>

## Tester la stationarité

<div style="margin-top: -1.5em;"></div>

### ADF {urca}

<div style="margin-top: -1.5em;"></div>

```{r}
w0=ur.df(x, type = "none", selectlags = "AIC")
summary(w0)@teststat ; summary(w0)@cval
w1=ur.df(diff(x), type = "none", selectlags = "AIC")
summary(w1)@teststat ; summary(w1)@cval
w2=ur.df(diff(diff(x)), type = "none", selectlags = "AIC")
summary(w2)@teststat ; summary(w2)@cval
```

---

## Changement structurel 

<!-- <div style="margin-top: -1.5em;"></div> -->

```{r}
za_test = ur.za(x)
za_summary <- summary(za_test)
za_summary@teststat
za_summary@cval
za_summary@bpoint
```

---

## Attention - ruptures structurelles

Tester la présence de racines unitaires en présence de ruptures structurelles, par exemple, Perron (1989).

Un processus I(0) avec une rupture est difficile à distinguer d'un processus I(1). Un grand coefficient sur la variable dépendante retardée (LDV) est nécessaire pour décaler vers une nouvelle moyenne.

Exemple :

Générer des données données par :

$$\begin{aligned}
y_{t} &= 0.5 y_{t-1} + \varepsilon_{t} \quad\quad\;\;\, \text{pour}     \quad t = 1, \ldots, 50\\
      &= 4 + 0.5 y_{t-1} + \varepsilon_{t} \quad \text{pour} \quad t = 51, \ldots, 100
\end{aligned}$$

où  $\varepsilon_{t} \sim \mathsf{NID}(0, 1)$

Ajuster un modèle AR(1) aux données :

$$y_{t} = \underset{(0.0000)}{0.9606}  y_{t-1} + \underset{(0.1060)}{3.8998}$$
(pvalues)

Un coefficient proche de l'unité sur la variable dépendante retardée (LDV).

---

## Processus AR(1) avec changement structurel

```{r, echo=FALSE}
set.seed(123)  # Pour la reproductibilité
n <- 100  # Nombre total de périodes
break_point <- 50  # Point de rupture

# Simuler la série temporelle avec une rupture structurelle
y <- numeric(n)
y[1] <- rnorm(1)  # Valeur initiale

for (t in 2:break_point) {
  y[t] <- 0.5 * y[t-1] + rnorm(1)
}

for (t in (break_point+1):n) {
  y[t] <- 4+ 0.5 * y[t-1] + rnorm(1)
}
par(mfrow=c(1,1))
tsplot(y)
```

---

## Ajustment AR(1)

```{r}
fit <- arima(y, order = c(1, 0, 0))
print(fit)
```

```{r, echo=F}
# Extraire les coefficients et les résidus
coefs <- coef(fit)

# Afficher les coefficients et les résidus
print(coefs)
std_errors = sqrt(diag(fit$var.coef))
p_values = 2 * (1 - pnorm(abs(coefs / std_errors)))
print("p_values")
print(p_values)
#Résidus et serie ajustée
yres = residuals(fit)
yfit = y - yres
```

---

## Graph des $y_t$, $\widehat{y_t}$ et $\varepsilon_t$

<div style="margin-top: -1.5em;"></div>

```{r fig.width = 10, fig.height = 4.0}
par(mfrow = c(2, 3))
tsplot(y)
points(yfit, type = "l", col = 2, lty = 2)
plot(y, yfit)
plot(yres, main = "Residuals")
acf(yres, main = "ACF of Residuals")
pacf(yres, main = "PACF of Residuals")
qqnorm(yres, main = "QQ Plot of Residuals")
qqline(yres)
```

---

# Conclusion

Les tests de racine unitaire - les estimateurs sont standard - OLS

MAIS les distributions limites de LR, W, t et F diffèrent des distributions standard $\chi^2$, $\mathsf{N}(0, 1)$

Fonctionnelles des mouvements brownien - Dickey-Fuller

Les distributions sont tabulées et simples à simuler

Problème - les tests de racine unitaire ont une faible puissance lorsque $\alpha \neq 1$ mais proche de 1

Difficile d'inférer les propriétés à long terme à partir d'un échantillon fini

Pour aller plus loin, étendre au cadre multivarié - cointégration
