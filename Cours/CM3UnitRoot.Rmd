---
title: "Racines unitaires"
author: "Chapitre 3"
date: "Économétrie des séries temporelles"
output:
  beamer_presentation:
    number_sections: true
    toc: true
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{amssymb}
  - \setbeamertemplate{footline}{\hfill\raisebox{2pt}[0pt][0pt]\scriptsize{Économétrie des         Séries Temporelles - Chapitre 3}\hspace*{10pt} {\insertframenumber{} /                     \inserttotalframenumber}\hspace*{2pt}
    }
fontsize: 8pt
editor_options: 
  markdown: 
    wrap: 72
---

<!-- --- -->

<!-- title: "Racines unitaires" -->

<!-- author: "Chapitre 3" -->

<!-- date: "Économétrie des séries temporelles" -->

<!-- output: -->

<!--   xaringan::moon_reader: -->

<!--     css: [default, hygge-duke, hygge, custom_colors.css, custom_footer.css] -->

<!--     lib_dir: libs -->

<!--     nature: -->

<!--       ratio: "14:9" -->

<!--       highlightStyle: github -->

<!--       highlightLines: true -->

<!--       countIncrementalSlides: false -->

<!--       titleSlideClass: ["center", "middle", "my-title"] -->

<!--     mathjax: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" -->

<!-- --- -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  error = TRUE,
  fig.width = 10, 
  fig.height = 5,
  dpi = 600,
  fig.keep ="last",
  message = FALSE,
  warning = FALSE,
  results = "markup"
)
```

<!-- layout:true -->

<!-- <div class="footer"><span>Économétrie des Séries Temporelles - Chapitre 3</span></div> -->

<!-- comme \vspace{} -->

<!-- <div style="margin-top: -0.5em;"></div> -->

------------------------------------------------------------------------

# Structure du cours

Marche aléatoire et marche aléatoire avec dérive

Processus stationnaires de tendance versus processus stationnaires de
différence

Régression fallacieuse

Test de racine unitaire

Tests de Dickey-Fuller

Tests de Dickey-Fuller augmentés

Exemple empirique

Conclusion

------------------------------------------------------------------------

# Marche aléatoire

De nombreuses séries temporelles macroéconomiques apparaissent non
stationnaires lorsqu'elles sont analysées par des modèles AR :

$$y_{t} = \alpha y_{t-1} + \varepsilon_{t}, \quad \varepsilon_{t} \sim \mathsf{NID}(0, \sigma^{2})$$

**Marche aléatoire** ($\alpha = 1$) :

$$y_{t} = y_{0} + \sum_{j=1}^{t} \varepsilon_{j}$$

Persistance des chocs aléatoires – **mémoire infinie**

La marche aléatoire est courante dans les marchés financiers et des
changes bilatéraux – taux de change USD/GBP.

**Marche aléatoire** – ne peut pas exploiter l'historique passé de
$(y_{t}, \varepsilon_{t})$ pour systématiquement réaliser des profits en
spéculant sur le taux de change futur.

Si $\alpha > 1$, $y_{t}$ est explosif.

------------------------------------------------------------------------

## Taux de change USD/GBP

```{r libs3, echo=FALSE}
library(fredr) #https://fred.stlouisfed.org/docs/api/api_key.html
fredr_set_key("229661c3a1cbd67e2388fafb768ca882")
library(astsa)
library(xts)
library(quantmod)
library(tseries)
library(lmtest)
library(urca)
```

```{r}
GBP <- getSymbols(Symbols = 'GBP=X', src = 'yahoo', 
                  from='2004-12-01', to='2024-12-01', auto.assign = FALSE)
GBP$Date = as.Date.character(GBP$Date, format = "%Y/%m/%d")
GBP.date <- GBP$Date 
GBP.p <- GBP$GBP.X.Close 
plot(GBP.p, main="GBP=X")
```

------------------------------------------------------------------------

## Marche aléatoire et processus explosif

```{r, echo=F}
par(mfrow=c(3,1))
x = arima.sim(list(order = c(1,0,0), ar = 0.95), n = 200)
tsplot(x, main= expression("AR(1), "~alpha~ "= 0.95"))
x = arima.sim(list(order = c(1,0,0), ar = 0.9999), n = 200)
tsplot(x, main = expression("Marche aléatoire, "~alpha~ "= 1"))
# Définir les paramètres
set.seed(123)  # Pour la reproductibilité
T <- 200  # Nombre total de périodes
ar_coeff <- 1.2  # Coefficient AR

# Initialiser la série temporelle
yt <- numeric(T)
yt[1] <- rnorm(1)  # Valeur initiale

# Simuler la série temporelle avec un processus explosif
for (t in 2:T) {
  yt[t] <- ar_coeff * yt[t-1] + rnorm(1)
}

# Tracer la série temporelle
plot(yt, type = "l", col = "blue", 
main = expression("Process AR(1) explosif, "~alpha~ "> 1"), ylab = "", xlab = "")
```

------------------------------------------------------------------------

# Marche aléatoire avec dérive

$$y_{t} = \mu + y_{t-1} + \varepsilon_{t}, \quad \varepsilon_{t} \sim \mathsf{NID}(0, \sigma^{2})$$

$\mu$ est le terme de dérive.

Par substitution récursive :

$$y_{t} = y_{0} + \mu t + \sum_{j=1}^{t} \varepsilon_{j}$$

La tendance déterministe est représentée par $\mu t$.

$$\mathsf{E}(y_{t}) = y_{0} + \mu t$$
$$\mathsf{V}(y_{t}) = \gamma_{t}(0) = t \sigma^{2}$$
$$\text{cov}(y_{t}, y_{t-h}) = \gamma_{t}(h) = (t - h) \sigma^{2}$$

------------------------------------------------------------------------

## Marche aléatoire avec dérive - graph

```{r, echo=F}
# Définir les paramètres
set.seed(123)  # Pour la reproductibilité
T <- 500  # Nombre total de périodes
y0 <- 0  # Valeur initiale
mu <- 0.4  # Coefficient de tendance

# Initialiser la série temporelle
yt <- numeric(T)
yt[1] <- y0  # Valeur initiale

# Simuler le processus
for (t in 2:T) {
  yt[t] <- yt[t-1] + mu + rnorm(1)
}

# Tracer la série temporelle
par(mfrow=c(2,1))
tsplot(yt, type = "l", col = "blue", main = "Process simulé", ylab = "", xlab = "")
tsplot(diff(yt), type = "l", col = "blue", main = "Process différencié simulé", ylab = "", xlab = "")
abline(h = mean(diff(yt)), col = "red", lty = 2, lwd = 2)
legend("topright", legend = "Moyenne", col = "red", lty = 2, lwd = 2, bty = "n")
```

------------------------------------------------------------------------

# Processus stationnaires de différence (DS) et de tendance (TS)

Les séries temporelles non stationnaires peuvent être classées en deux grandes catégories :

- **Processus stationnaires de différence (DS - Difference Stationary)**
- **Processus stationnaires de tendance (TS - Trend Stationary)**

---

## Processus stationnaires de différence (DS)

Un processus DS est non stationnaire, mais peut devenir stationnaire après avoir pris une ou plusieurs différences (\(d\)).

### Exemple : Marche aléatoire avec ou sans dérive (cf. slides précédents)
$$y_t = \mu + y_{t-1} + \varepsilon_t, \quad \varepsilon_t \sim \mathsf{NID}(0, \sigma^2)$$

1. Si \(\mu = 0\), c’est une **marche aléatoire pure**.
2. Si \(\mu \neq 0\), c’est une **marche aléatoire avec dérive**.

Après différenciation (\(\Delta y_t = y_t - y_{t-1}\)) :
$$\Delta y_t = \mu + \varepsilon_t.$$

- \(\Delta y_t\) est **stationnaire** (variance et moyenne constantes dans le temps).

---

## Processus stationnaires de tendance (TS)

Un processus TS est stationnaire autour d’une tendance déterministe. Contrairement aux processus DS, la non-stationnarité est éliminée en supprimant la **tendance déterministe**.

### Exemple : Modèle TS
$$y_t = \mu + \beta t + u_t, \quad u_t \sim \mathsf{NID}(0, \sigma^2)$$

1. \(\mu\) est une constante (intercept),
2. \(\beta t\) représente une **tendance déterministe**,
3. \(u_t\) est un processus **stationnaire** autour de la tendance.

La série devient stationnaire après dé-trendation (\(y_t - \mathsf{E}[y_t]\)) :
$$y_t - (\mu + \beta t) = u_t.$$

--- 

## Stationnaire en tendance

```{r, echo=F}
# Définir les paramètres
set.seed(123)  # Pour la reproductibilité
T <- 500       # Nombre total de périodes
y0 <- 0        # Valeur initiale
mu <- 0.4      # Coefficient de tendance

# Simuler le processus TS
t_seq <- 1:T                     # Périodes de temps
trend <- y0 + mu * t_seq         # Tendance déterministe
noise <- rnorm(T, mean = 0, sd = 9)  # Bruit blanc avec forte variabilité pour illustrer
yt <- trend + noise              # Processus TS : tendance + bruit

# Dé-trendation : supprimer la tendance déterministe
yt_detrended <- yt - trend

# Tracer la série temporelle
par(mfrow = c(2, 1))  # Deux graphiques empilés

# Graphique 1 : Série avec tendance
plot.ts(yt, type = "l", col = "blue", main = "Processus stationnaire en tendance (TS)", 
        ylab = "y_t", xlab = "Temps")
lines(trend, col = "red", lty = 2)  # Ajouter la tendance déterministe
legend("topleft", legend = c("y_t", "Tendance"), col = c("blue", "red"), 
       lty = c(1, 2), bty = "n", cex = 0.8)

# Graphique 2 : Série dé-trendée
plot.ts(yt_detrended, type = "l", col = "blue", 
        main = "Série dé-trendée (stationnaire)", ylab = "y_t - tendance", xlab = "Temps")
abline(h = 0, col = "red", lty = 2)  # Moyenne de la série dé-trendée
legend("topleft", legend = c("Série dé-trendée", "Moyenne (h=0)"), 
       col = c("blue", "red"), lty = c(1, 2), bty = "n", cex = 0.8)

```

--- 

## Différences principales entre DS et TS

| **Caractéristique**              | **Processus DS**                       | **Processus TS**                    |
|----------------------------------|----------------------------------------|-------------------------------------|
| Source de non-stationnarité      | Tendance stochastique                  | Tendance déterministe               |
| Transformation pour stationnarité| Différenciation                        | Dé-trendation                       |
| Exemple                          | Marche aléatoire, marche aléatoire avec dérive | Série avec tendance linéaire         |
| Stationnarité                    | Stationnaire après \(d\) différences   | Stationnaire après suppression de la tendance |


------------------------------------------------------------------------

## Types de modèles univariés

$$y_{t} = \mu + \alpha y_{t-1} + \beta t + \varepsilon_{t}, \quad \varepsilon_{t} \sim \mathsf{NID}(0, \sigma^{2})$$

| Paramètre | Description | Propriétés |
|----------------------|--------------------------|------------------------|
| $\mu \neq 0; \mid\alpha\mid < 1; \beta \neq 0$ | Tendance déterministe avec composante AR(1) stationnaire | $\mathsf{I}(0)$ |
| $\mu \neq 0; \alpha = 1; \beta \neq 0$ | Marche aléatoire avec dérive et tendance quadratique | $\mathsf{I}(1)$ |
| $\mu \neq 0; \alpha = 1; \beta = 0$ | Marche aléatoire avec dérive | $\mathsf{I}(1)$ |
| $\mu \neq 0; \alpha = 0; \beta \neq 0$ | Tendance déterministe | $\mathsf{I}(0)$ |
| $\mu = 0; \alpha = 1; \beta = 0$ | Marche aléatoire pure | $\mathsf{I}(1)$ |

------------------------------------------------------------------------

## Pour le cas marche aléatoire avec dérive et tendance quadratique

$$y_{t} = \mu + \alpha y_{t-1} + \beta t + \varepsilon_{t}$$

Donc :

$$\Delta y_{t} = \mu + \beta t + \varepsilon_{t} \sim \mathsf{I}(0)$$

Si
$\Delta y_{t} \sim \mathsf{I}(0) \Rightarrow \sum_{i=1}^{t} [\Delta y_{i}] \sim \mathsf{I}(1)$

$$y_{t} = \mu t + \beta t^{2} + \sum_{i=1}^{t} \varepsilon_{i}$$

Donc, une tendance quadratique dans le niveau.

Peu probable de se produire en pratique, sauf localement.

------------------------------------------------------------------------

### Processus à racine unitaire

Le processus à racine unitaire est donné par :

$$\begin{aligned}
y_{t} &= \mu + y_{t-1} + \varepsilon_{t} \\
&= \mu t + y_{0} + \sum_{j=1}^{t} \varepsilon_{j}
\end{aligned}$$

où $\sum_{j=1}^{t} \varepsilon_{j}$ est une marche aléatoire ;

$y_{0}$ est la condition initiale ;

et $\mu t$ est le terme de dérive.

**Problème :** La théorie de la distribution pour les estimateurs et les
statistiques de test diffère du cas transversal/stationnaire -- en
raison de la dépendance temporelle.

# 2. Régression fallacieuse

Deux séries non apparentées $\mathsf{I}(1)$.

Régression de $y$ sur $z$ $\Rightarrow$ relation apparemment
significative.

Transformations pour induire la stationnarité :

-   Différenciation

-   Suppression de la tendance déterministe

-   Cointégration

------------------------------------------------------------------------

### Rappel

Régression OLS standard pour des données stationnaires :
\begin{equation}
y_{t} = \beta x_{t} + u_{t}, \quad u_{t} \sim \mathsf{NID}(0, \sigma^{2})
\end{equation}

$\widehat{\beta} = \left[\sum_{t=1}^{T} x_{t}^{2}\right]^{-1} \sum_{t=1}^{T} x_{t} y_{t} = \left(X^{\prime} X\right)^{-1} \left(X^{\prime} y\right)$

$\mathsf{E}\left(\widehat{\beta} | x_{1}, \cdots, x_{T}\right) = \beta$

$\mathsf{V}\left(\widehat{\beta} | x_{1}, \cdots, x_{T}\right) = \sigma^{2} \left(X^{\prime} X\right)^{-1}$

------------------------------------------------------------------------

### Résultats asymptotiques

\begin{equation}
\widehat{\beta} - \beta = \frac{\frac{1}{T} \sum_{t=1}^{T} x_{t} u_{t}}{\frac{1}{T} \sum_{t=1}^{T} x_{t}^{2}}
\end{equation}

En utilisant la loi faible des grands nombres : $$\begin{aligned}
\frac{1}{T} \sum_{t=1}^{T} x_{t} u_{t} &\overset{p}{\longrightarrow} \mathsf{E}\left(x_{t} u_{t}\right) = 0 \nonumber \\
\frac{1}{T} \sum_{t=1}^{T} x_{t}^{2} &\overset{p}{\longrightarrow} \mathsf{E}\left(x_{t}^{2}\right) = \mathbf{Q} \neq 0
\end{aligned}$$

Donc : \begin{equation}
\widehat{\beta} - \beta \overset{p}{\longrightarrow} \mathbf{Q}^{-1} \times 0 = 0
\end{equation} ce qui établit la convergence.

------------------------------------------------------------------------

### Résultats asymptotiques (suite)

\begin{equation}
\sqrt{T} \left(\widehat{\beta} - \beta\right) = \sqrt{T} \frac{\frac{1}{T} \sum_{t=1}^{T} x_{t} u_{t}}{\frac{1}{T} \sum_{t=1}^{T} x_{t}^{2}}
\end{equation}

Comme
$\frac{1}{T} \sum_{t=1}^{T} x_{t}^{2} \overset{p}{\longrightarrow} \mathbf{Q}$
(une constante) et $\frac{1}{\sqrt{T}} \sum_{t=1}^{T} x_{t} u_{t}$ a une
distribution normale limite, alors
$\sqrt{T} \left(\widehat{\beta} - \beta\right)$ est asymptotiquement
normalement distribué.

La preuve de la convergence nécessite
$\mathsf{plim}\left(1/T\right) \left(X^{\prime} X\right) = \mathbf{Q}$
-- avec l'augmentation de l'information de l'échantillon, les moments de
l'échantillon convergent vers les moments de la population (fixes).

Par conséquent, les moments de la population doivent être stationnaires.

**Que se passe-t-il si les données ne sont pas stationnaires ?**

------------------------------------------------------------------------

### Données non stationnaires -- régression fallacieuse

**DGP** :

$$\begin{aligned}
y_{t} &= y_{t-1} + u_{t}  \\
x_{t} &= x_{t-1} + v_{t}  \\
\mathsf{E}(u_{t} v_{s}) &= 0, \forall t, s; \quad \mathsf{E}(u_{t} u_{k}) = \mathsf{E}(v_{t} v_{k}) = 0, \forall k \neq 0 \\
y_{0} &= x_{0} = 0 \nonumber
\end{aligned}$$

c'est-à-dire, $x_{t}$ et $y_{t}$ sont des marches aléatoires non
corrélées.

Hypothèse économique : \begin{equation}
y_{t} = \beta_{0} + \beta_{1} x_{t} + \varepsilon_{t}
\end{equation}

Nous devrions trouver
$\beta_{1} = \frac{\partial y_{t}}{\partial x_{t}} \overset{p}{\longrightarrow} 0$.

------------------------------------------------------------------------

Yule (1926) -- les corrélations fallacieuses peuvent persister dans de
grands échantillons.

Sous $H_{0} : \beta_{1} = 0$, nous nous attendrions à ce que
$\mathsf{E}(\widehat{\beta}_{1}) = 0$.

Pour des régresseurs non stationnaires, cela ne tient pas.

Les statistiques t conventionnelles sur $\widehat{\beta}_{0}$ et
$\widehat{\beta}_{1}$ n'ont pas de distributions t.

Elles divergent en distribution à mesure que $T$ augmente. Par
conséquent, pour $H_{0} : \beta_{1} = 0$, nous rejetterions plus
fréquemment à mesure que $T$ augmente.

Problème d'équilibre : $y_{t} \sim {\mathsf I}(1)$ et $\varepsilon_{t}$
supposé $\sim {\mathsf I}(0)$.

Alors $y_{t} = \beta_{0} + \beta_{1} x_{t} + \varepsilon_{t}$ pourrait
être bien défini pour $\beta_{1}$ non nul comme
$x_{t} \sim {\mathsf I}(1)$.

Lorsque $\beta_{1} = 0$, $\{\varepsilon_{t}\}$ doit être
${\mathsf I}(1)$ -- ce qui viole les hypothèses.

------------------------------------------------------------------------

Pour les données non stationnaires, $\widehat{\beta}_{1}$ ne converge
pas vers $0$, mais converge plutôt vers une variable aléatoire.

$$\widehat{\beta}_{1} = \left[T^{-1} \sum_{t=1}^{T} \left(x_{t} - \overline{x}\right)^{2}\right]^{-1} T^{-2} \sum_{t=1}^{T} \left(x_{t} - \overline{x}\right) \left(y_{t} - \overline{y}\right)$$

Le numérateur et le dénominateur convergent faiblement vers des
fonctionnelles du mouvement brownien.

Comme $x \sim {\mathsf I}(1)$, la variance de l'échantillon $= f(T)$.

Régression déséquilibrée, $\varepsilon \sim {\mathsf I}(1)$, la variance
de l'échantillon $= f(T)$.

$\widehat{\beta}_{0}$ et $\widehat{\beta}_{1}$ minimisent
$\widehat{\sigma}^{2}_{\varepsilon}$.

On observe que
$\widehat{\sigma}_{\beta_{1}} = \widehat{\sigma}_{\varepsilon} \left(\sum_{t=1}^{T} \left(x_{t} - \overline{x}\right)^{2}\right)^{-\frac{1}{2}}$
est une fonction de $T$.

Par conséquent,
${\mathsf t} = \frac{\widehat{\beta}_{1}}{\widehat{\sigma}_{\beta_{1}}}$
diverge lorsque $T \rightarrow \infty$.

------------------------------------------------------------------------

# 3. Tests de racine unitaire

## Dickey-Fuller test

$$y_{t} = \alpha y_{t-1} + \varepsilon_{t}, \quad \varepsilon_{t} \sim \mathsf{IID}(0, \sigma^{2})$$

Définissons $\gamma = \alpha - 1$:

$$\Delta y_{t} = \gamma y_{t-1} + \varepsilon_{t}$$

Si racine unitaire, $\alpha = 1 \Rightarrow \gamma = 0$.

Par conséquent, nous souhaitons tester :

$$\mathsf{H_{0}} : \gamma = 0$$

Cela suggère OLS et le rejet de l'hypothèse nulle si $\widehat{\gamma}$
est significativement négatif.

**Problème** : il est nécessaire de connaître la distribution de la
statistique de test sous l'hypothèse nulle.

Sous l'hypothèse nulle, la série est non stationnaire. Cela implique que
la statistique de test n'est pas distribuée asymptotiquement selon une
loi normale.

------------------------------------------------------------------------

## Dickey-Fuller test (suite)

Considérons l'estimateur de $\widehat{\gamma}$ lorsque $\gamma = 0$ :

$$\widehat{\gamma} = \frac{\frac{1}{T} \sum_{t=1}^{T} y_{t-1} \varepsilon_{t}}{\frac{1}{T} \sum_{t=1}^{T} y_{t-1}^{2}}$$

En utilisant $y_{t-1} = \sum_{j=1}^{t-1} \varepsilon_{j}$ :

$$T \widehat{\gamma} = \frac{\frac{1}{T} \sum_{t=1}^{T} \left(\sum_{j=1}^{t-1} \varepsilon_{j}\right) \varepsilon_{t}}{\frac{1}{T^{2}} \sum_{t=1}^{T} \left(\sum_{j=1}^{t-1} \varepsilon_{j}\right)^{2}} \label{DF1}$$

$\sum_{j=1}^{t-1} \varepsilon_{j}$ n'est pas ergodique, ni
$\mathsf{IID}$, donc la loi des grands nombres (LLN) ne s'applique pas.

$\sum_{j=1}^{t-1} \varepsilon_{j} \varepsilon_{t}$ n'est pas une
séquence de différences martingales satisfaisant les hypothèses pour le
théorème central limite (CLT).

------------------------------------------------------------------------

## Dickey-Fuller test (suite)

Les distributions limites de $T \widehat{\gamma}$ ont été simulées par
Dickey et Fuller (JASA, 1979) et MacKinnon (1991).

Ces distributions sont appelées distributions de Dickey-Fuller.

Les logiciels économétriques produisent des valeurs critiques à partir
de surfaces de réponse.

Le test de racine unitaire permet de distinguer entre les données
$\textsf{I}(0)$ et $\textsf{I}(1)$.

Mais différentes hypothèses nulles sont possibles en fonction des termes
déterministes.

Par conséquent, différentes statistiques de test en fonction des
hypothèses nulles.

------------------------------------------------------------------------

Trois cas à considérer :

| Test | Modèle | Hypothèse |
|----------------|----------------------------------|----------------------|
| $\widehat{\tau}$ | $\Delta y_{t} = \gamma y_{t-1} + \varepsilon_{t}$ | $H_{0} : \gamma = 0$ |
| $\widehat{\tau}_{\mu}$ | $\Delta y_{t} = \mu + \gamma y_{t-1} + \varepsilon_{t}$ | $H_{0} : \mu = 0; \gamma = 0$ |
| $\widehat{\tau}_{\beta}$ | $\Delta y_{t} = \mu + \gamma y_{t-1} + \beta t + \varepsilon_{t}$ | $H_{0} : \beta = 0; \gamma = 0$ |

Les valeurs critiques dépendent de la spécification des hypothèses
nulles et alternatives.

Un test unilatéral est généralement utilisé pour maximiser la puissance
:

$\mathsf{H}_{0} : \gamma = 0$ versus $\mathsf{H}_{1} : \gamma < 0$; car
$\gamma > 0 \Rightarrow$ processus explosif.

| Distribution          | 2.5%  | 5%    | 10%   | 50% | 90%   | 95%   | 97.5% |
|-----------------------|-------|-------|-------|-----|-------|-------|-------|
| $\mathsf{N}(0,1)$     | -1.96 | -1.64 | -1.28 | 0   | 1.28  | 1.64  | 1.96  |
| $\mathsf{DF}_{\mu}$   | -3.12 | -2.86 | -2.57 |     | -0.44 | -0.07 | 0.23  |
| $\mathsf{DF}_{\beta}$ | -3.66 | -3.41 | -3.12 |     | -1.25 | -0.94 | -0.66 |

------------------------------------------------------------------------

## Procédure de test

1.  Évaluer les données graphiquement.

2.  Si une tendance est probable, tester l'hypothèse conjointe
    $\mathsf{H}_{0} : \gamma = 0 ; \beta = 0$. La non-rejection
    $\Rightarrow$ processus DS sans tendance déterministe.

3.  $\mathsf{H}_{0}$ pourrait être rejeté en raison de :

    -   $\gamma \neq 0 ; \beta = 0$ : Aucun mécanisme générant une
        tendance.
    -   $\gamma = 0 ; \beta \neq 0$ : Correspond à une tendance
        quadratique dans le DGP.
    -   $\gamma \neq 0 ; \beta \neq 0$ : Tendance stationnaire
        ($\gamma < 0$).

4.  Si les données ne présentent pas de tendance, tester l'hypothèse
    conjointe $\mathsf{H}_{0} : \gamma = 0 ; \mu = 0$.

5.  $\mathsf{H}_{0}$ pourrait être rejeté en raison de :

    -   $\gamma \neq 0 ; \mu = 0$ : Stationnarité.
    -   $\gamma = 0 ; \mu \neq 0$ : Donnera une dérive notable.
    -   $\gamma \neq 0 ; \mu \neq 0$ : Distribution normale standard
        asymptotique.

6.  Si la moyenne est zéro, sans tendance, tester l'hypothèse
    $\mathsf{H}_{0} : \gamma = 0$.

------------------------------------------------------------------------

## Augmented Dickey-Fuller Test

Le test est considéré sur la base d'un processus AR(1). Si le DGP est un
processus AR(p) :

$$y_{t} = \mu + \alpha_{1} y_{t-1} + \cdots + \alpha_{p} y_{t-p} + \varepsilon_{t}$$

mais un modèle AR(1) est ajusté :

$$y_{t} = \mu + \alpha_{1} y_{t-1} + \nu_{t}$$

alors :

$$\nu_{t} = \alpha_{2} y_{t-2} + \cdots + \alpha_{p} y_{t-p} + \varepsilon_{t}$$

ce qui génère une autocorrélation sérielle.

Ajuster un modèle AR(p) en utilisant une stratégie de recherche générale
à spécifique. Commencer avec un grand $p$ et tester vers le bas pour
s'assurer qu'il n'y a pas d'autocorrélation sérielle.

------------------------------------------------------------------------

Considérons un processus AR(2) :

$$\begin{aligned}
y_{t} &= \mu + \alpha_{1} y_{t-1} + \alpha_{2} y_{t-2} + \varepsilon_{t}  \\
&= \mu + \left(\alpha_{1} + \alpha_{2}\right) y_{t-1} - \alpha_{2} \left(y_{t-1} - y_{t-2}\right) + \varepsilon_{t}  \\
\Delta y_{t} &= \mu + \gamma y_{t-1} + \phi_{1} \Delta y_{t-1} + \varepsilon_{t}
\end{aligned}$$

où $\gamma = \alpha_{1} + \alpha_{2} - 1$ et $\phi_{1} = -\alpha_{2}$.

Le modèle de Dickey-Fuller est augmenté par $\Delta y_{t-1}$.

Tester $\mathsf{H}_{0} : \gamma = 0$ pour $\widehat{\tau}_{\mu}$ comme
avant.

Modèle ADF(p) avec intercept et tendance :

$$\Delta y_{t} = \mu + \gamma y_{t-1} + \sum_{j=1}^{p-1} \phi_{j} \Delta y_{t-j} + \beta t + \varepsilon_{t}$$

::: {style="margin-top: -1em;"}
:::

où

::: {style="margin-top: -1em;"}
:::

$$\begin{aligned}
\gamma &= \alpha_{1} + \alpha_{2} + \cdots + \alpha_{p} - 1,  \\
\phi_{1} &= -\left(\alpha_{2} + \cdots + \alpha_{p}\right)  \\
& \;\; \vdots \\
\phi_{p-1} &= -\alpha_{p}
\end{aligned}$$

------------------------------------------------------------------------

# 4. Illustration empirique

## Données

```{r}
USunrate = fredr(series_id = "UNRATE",
           observation_start = as.Date("1948-01-01"),
           observation_end = as.Date("2023-10-01"),
           frequency = "a")
x = USunrate$value
xDate = USunrate$date
```

------------------------------------------------------------------------

## $USunrate$, $\Delta USunrate$, $\Delta^{2} USunrate$

::: {style="margin-top: -1.5em;"}
:::

```{r fig.width = 10, fig.height = 4.0}
par(mfrow=c(3,1))
tsplot(xDate,x, 
main="Taux de chomage", ylab = "%", xlab='')
# Si on différencie la série 1 fois :
tsplot(xDate[-1],diff(x),main= expression(Delta ~ "Taux de chomage aux USA"),
       ylab = "%", xlab='')
# Si on différencie la série 2 fois :
tsplot(xDate[-1][-1],diff(diff(x)),main= 
         expression(Delta^2 ~ "Taux de chomage aux USA"),ylab = "%",xlab='')
```

------------------------------------------------------------------------

::: {style="margin-top: -1.5em;"}
:::

## Tester la stationarité

::: {style="margin-top: -1.5em;"}
:::

### ADF {tseries}

::: {style="margin-top: -1.5em;"}
:::

```{r}
adf.test(x)
adf.test(diff(x))
adf.test(diff(diff(x)))
```

------------------------------------------------------------------------

::: {style="margin-top: -1.5em;"}
:::

## Tester la stationarité

::: {style="margin-top: -1.5em;"}
:::

### ADF {urca}

::: {style="margin-top: -1.5em;"}
:::

```{r}
w0=ur.df(x, type = "none", selectlags = "AIC")
summary(w0)@teststat ; summary(w0)@cval
w1=ur.df(diff(x), type = "none", selectlags = "AIC")
summary(w1)@teststat ; summary(w1)@cval
w2=ur.df(diff(diff(x)), type = "none", selectlags = "AIC")
summary(w2)@teststat ; summary(w2)@cval
```

------------------------------------------------------------------------

## Changement structurel

<!-- <div style="margin-top: -1.5em;"></div> -->

```{r}
za_test = ur.za(x)
za_summary <- summary(za_test)
za_summary@teststat
za_summary@cval
za_summary@bpoint
```

------------------------------------------------------------------------

## Attention - ruptures structurelles

Tester la présence de racines unitaires en présence de ruptures
structurelles, par exemple, Perron (1989).

Un processus I(0) avec une rupture est difficile à distinguer d'un
processus I(1). Un grand coefficient sur la variable dépendante retardée
(LDV) est nécessaire pour décaler vers une nouvelle moyenne.

Exemple :

Générer des données données par :

$$\begin{aligned}
y_{t} &= 0.5 y_{t-1} + \varepsilon_{t} \quad\quad\;\;\, \text{pour}     \quad t = 1, \ldots, 50\\
      &= 4 + 0.5 y_{t-1} + \varepsilon_{t} \quad \text{pour} \quad t = 51, \ldots, 100
\end{aligned}$$

où $\varepsilon_{t} \sim \mathsf{NID}(0, 1)$

Ajuster un modèle AR(1) aux données :

$$y_{t} = \underset{(0.0000)}{0.9606}  y_{t-1} + \underset{(0.1060)}{3.8998}$$
(pvalues)

Un coefficient proche de l'unité sur la variable dépendante retardée
(LDV).

------------------------------------------------------------------------

## Processus AR(1) avec changement structurel

```{r, echo=FALSE}
set.seed(123)  # Pour la reproductibilité
n <- 100  # Nombre total de périodes
break_point <- 50  # Point de rupture

# Simuler la série temporelle avec une rupture structurelle
y <- numeric(n)
y[1] <- rnorm(1)  # Valeur initiale

for (t in 2:break_point) {
  y[t] <- 0.5 * y[t-1] + rnorm(1)
}

for (t in (break_point+1):n) {
  y[t] <- 4+ 0.5 * y[t-1] + rnorm(1)
}
par(mfrow=c(1,1))
tsplot(y)
```

------------------------------------------------------------------------

## Ajustment AR(1)

```{r}
fit <- arima(y, order = c(1, 0, 0))
print(fit)
```

```{r, echo=F}
# Extraire les coefficients et les résidus
coefs <- coef(fit)

# Afficher les coefficients et les résidus
print(coefs)
std_errors = sqrt(diag(fit$var.coef))
p_values = 2 * (1 - pnorm(abs(coefs / std_errors)))
print("p_values")
print(p_values)
#Résidus et serie ajustée
yres = residuals(fit)
yfit = y - yres
```

------------------------------------------------------------------------

## Graph des $y_t$, $\widehat{y_t}$ et $\varepsilon_t$

::: {style="margin-top: -1.5em;"}
:::

```{r fig.width = 10, fig.height = 4.0}
par(mfrow = c(2, 3))
tsplot(y)
points(yfit, type = "l", col = 2, lty = 2)
plot(y, yfit)
plot(yres, main = "Residuals")
acf(yres, main = "ACF of Residuals")
pacf(yres, main = "PACF of Residuals")
qqnorm(yres, main = "QQ Plot of Residuals")
qqline(yres)
```

------------------------------------------------------------------------

# 5. Conclusion

Les tests de racine unitaire - les estimateurs sont standard - OLS

MAIS les distributions limites de LR, W, t et F diffèrent des
distributions standard $\chi^2$, $\mathsf{N}(0, 1)$

Fonctionnelles des mouvements brownien - Dickey-Fuller

Les distributions sont tabulées et simples à simuler

Problème - les tests de racine unitaire ont une faible puissance lorsque
$\alpha \neq 1$ mais proche de 1

Difficile d'inférer les propriétés à long terme à partir d'un
échantillon fini

Pour aller plus loin, étendre au cadre multivarié - cointégration
